{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "925b351b-ae66-4b94-a66c-604596f91b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "from geopy.geocoders import Nominatim\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns  # Assuming user has seaborn; if not, remove sns.set_style\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, LeakyReLU\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def vincenty(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"\n",
    "    Calculate the geodesic distance between two points on the Earth using Vincenty's formula (in km).\n",
    "    More accurate than Haversine as it accounts for Earth's ellipsoidal shape.\n",
    "    \"\"\"\n",
    "    # WGS-84 ellipsoid parameters\n",
    "    a = 6378137.0  # semi-major axis in meters\n",
    "    f = 1 / 298.257223563  # flattening\n",
    "    b = a * (1 - f)  # semi-minor axis\n",
    "\n",
    "    # Convert degrees to radians\n",
    "    phi1 = math.radians(lat1)\n",
    "    lambda1 = math.radians(lon1)\n",
    "    phi2 = math.radians(lat2)\n",
    "    lambda2 = math.radians(lon2)\n",
    "\n",
    "    # Reduced latitudes\n",
    "    U1 = math.atan((1 - f) * math.tan(phi1))\n",
    "    U2 = math.atan((1 - f) * math.tan(phi2))\n",
    "\n",
    "    L = lambda2 - lambda1  # longitude difference\n",
    "    lam = L  # initial approximation of lambda\n",
    "\n",
    "    sinU1 = math.sin(U1)\n",
    "    cosU1 = math.cos(U1)\n",
    "    sinU2 = math.sin(U2)\n",
    "    cosU2 = math.cos(U2)\n",
    "\n",
    "    iter_limit = 100\n",
    "    for _ in range(iter_limit):\n",
    "        sin_lam = math.sin(lam)\n",
    "        cos_lam = math.cos(lam)\n",
    "\n",
    "        sin_sigma = math.sqrt((cosU2 * sin_lam)**2 + (cosU1 * sinU2 - sinU1 * cosU2 * cos_lam)**2)\n",
    "        if sin_sigma == 0:\n",
    "            return 0.0  # coincident points\n",
    "\n",
    "        cos_sigma = sinU1 * sinU2 + cosU1 * cosU2 * cos_lam\n",
    "        sigma = math.atan2(sin_sigma, cos_sigma)\n",
    "\n",
    "        sin_alpha = (cosU1 * cosU2 * sin_lam) / sin_sigma\n",
    "        cos_sq_alpha = 1 - sin_alpha**2\n",
    "\n",
    "        if cos_sq_alpha == 0:\n",
    "            return (a * math.pi) / 1000  # antipodal points, approximate\n",
    "\n",
    "        cos_2sigma_m = cos_sigma - (2 * sinU1 * sinU2) / cos_sq_alpha\n",
    "\n",
    "        C = f / 16 * cos_sq_alpha * (4 + f * (4 - 3 * cos_sq_alpha))\n",
    "\n",
    "        lam_prev = lam\n",
    "        lam = L + (1 - C) * f * sin_alpha * (sigma + C * sin_sigma * (cos_2sigma_m + C * cos_sigma * (-1 + 2 * cos_2sigma_m**2)))\n",
    "\n",
    "        if abs(lam - lam_prev) < 1e-12:\n",
    "            break\n",
    "    else:\n",
    "        raise ValueError(\"Vincenty formula failed to converge after 100 iterations\")\n",
    "\n",
    "    u_sq = cos_sq_alpha * (a**2 - b**2) / (b**2)\n",
    "    A = 1 + u_sq / 16384 * (4096 + u_sq * (-768 + u_sq * (320 - 175 * u_sq)))\n",
    "    B = u_sq / 1024 * (256 + u_sq * (-128 + u_sq * (74 - 47 * u_sq)))\n",
    "\n",
    "    delta_sigma = B * sin_sigma * (cos_2sigma_m + B / 4 * (cos_sigma * (-1 + 2 * cos_2sigma_m**2) - B / 6 * cos_2sigma_m * (-3 + 4 * sin_sigma**2) * (-3 + 4 * cos_2sigma_m**2)))\n",
    "\n",
    "    s = b * A * (sigma - delta_sigma) / 1000  # distance in km\n",
    "\n",
    "    return s\n",
    "\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"\n",
    "    Calculate the great-circle distance between two points on the Earth (in km).\n",
    "    \"\"\"\n",
    "    R = 6371.0  # Earth radius in km\n",
    "    lat1_rad = np.radians(lat1)\n",
    "    lon1_rad = np.radians(lon1)\n",
    "    lat2_rad = np.radians(lat2)\n",
    "    lon2_rad = np.radians(lon2)\n",
    "    \n",
    "    dlat = lat2_rad - lat1_rad\n",
    "    dlon = lon2_rad - lon1_rad\n",
    "    \n",
    "    a = np.sin(dlat / 2)**2 + np.cos(lat1_rad) * np.cos(lat2_rad) * np.sin(dlon / 2)**2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "    \n",
    "    return R * c\n",
    "\n",
    "def get_or_create_geo_mapping(df):\n",
    "    \"\"\"\n",
    "    Loads geocoding from a file or creates it by calling an API.\n",
    "    \"\"\"\n",
    "    json_path = 'geo_mapping.json'\n",
    "    if os.path.exists(json_path):\n",
    "        print(f\"‚úÖ Found existing geocoding file at '{json_path}'. Loading...\")\n",
    "        with open(json_path, 'r') as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    print(f\"‚ö†Ô∏è No geocoding file found. Creating a new one using geopy...\")\n",
    "    print(\"This will take several minutes as it respects API rate limits.\")\n",
    "\n",
    "    geolocator = Nominatim(user_agent=\"property_matcher_app_v6\")\n",
    "    unique_locations = df['Location'].dropna().unique()\n",
    "    geo_mapping = {}\n",
    "\n",
    "    city_prefix_map = {\n",
    "        'A': 'Ahmedabad', 'P': 'Pune', 'G': 'Gandhinagar',\n",
    "        'S': 'Surat', 'V': 'Vadodara', 'B': 'Vadodara'\n",
    "    }\n",
    "\n",
    "    for location_str in unique_locations:\n",
    "        if not isinstance(location_str, str) or '-' not in location_str:\n",
    "            continue\n",
    "\n",
    "        parts = location_str.split('-', 1)\n",
    "        prefix = parts[0].strip()\n",
    "        area = parts[1].strip()\n",
    "        city = city_prefix_map.get(prefix, 'Ahmedabad')\n",
    "\n",
    "        query = f\"{area}, {city}, India\"\n",
    "        try:\n",
    "            location_data = geolocator.geocode(query, timeout=10)\n",
    "            if location_data:\n",
    "                geo_mapping[location_str] = (location_data.latitude, location_data.longitude)\n",
    "                print(f\"‚úÖ Found: {query} -> ({location_data.latitude:.4f}, {location_data.longitude:.4f})\")\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è Not Found: {query}\")\n",
    "            time.sleep(1)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error for '{query}': {e}\")\n",
    "\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(geo_mapping, f, indent=4)\n",
    "    print(f\"‚úÖ Geocoding complete. Saved to '{json_path}'.\")\n",
    "    return geo_mapping\n",
    "\n",
    "def run_data_pipeline():\n",
    "    \"\"\"\n",
    "    Loads, cleans, and preprocesses the property data, including geocoding.\n",
    "    Uses latitude and longitude directly as numerical features for better geographic similarity.\n",
    "    Adds a derived 'Price_Per_SqFt' feature.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        property_df = pd.read_csv('PropertyData.csv', low_memory=False)\n",
    "        print(\"‚úÖ Successfully loaded PropertyData.csv\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ùå Error: 'PropertyData.csv' not found.\")\n",
    "        return None, None, None, None\n",
    "\n",
    "    df = property_df.copy()\n",
    "    # Ensure necessary columns are included\n",
    "    features_to_use = [\n",
    "        'BHK', 'Property-Price', 'City1', 'Property-On-Floor', 'Property-Facing',\n",
    "        'Age-Of-Property', 'Super-Built-up-Construction-Area', 'Carpet-Construction-Area',\n",
    "        'Bathroom', 'Furniture-Details', 'Property-Status', 'Current-Status', 'Location', 'Parking-Details',\n",
    "        'No-Of-Lift-Per-Block', 'Service_Expiry_Date', 'Tag'\n",
    "    ]\n",
    "    df = df[features_to_use]\n",
    "    print(f\"‚úÖ Selected {len(features_to_use)} features for modeling.\")\n",
    "\n",
    "    # --- UPDATED: Three-State Status Calculation ---\n",
    "    print(\"\\nüóìÔ∏è Calculating property status (Active/Expired/Sold)...\")\n",
    "    \n",
    "    # Define what constitutes a 'Sold' property\n",
    "    sold_statuses = ['Sold-CD', 'Sold-Others', 'Rented-CD']\n",
    "    \n",
    "    # Define the reference date for checking expiry (updated to current date)\n",
    "    current_date = pd.to_datetime('2025-08-23')\n",
    "    \n",
    "    # Convert Service_Expiry_Date to datetime objects\n",
    "    df['Service_Expiry_Date'] = pd.to_datetime(df['Service_Expiry_Date'], errors='coerce', dayfirst=True)\n",
    "    \n",
    "    # Create the new reliable status column with three states using np.select\n",
    "    conditions = [\n",
    "        df['Property-Status'].isin(sold_statuses),\n",
    "        df['Service_Expiry_Date'] < current_date\n",
    "    ]\n",
    "    choices = ['Sold', 'Expired']\n",
    "    df['Calculated_Status'] = np.select(conditions, choices, default='Active')\n",
    "    \n",
    "    print(f\"‚úÖ Status calculation complete. Status distribution:\")\n",
    "    print(df['Calculated_Status'].value_counts())\n",
    "\n",
    "    geo_mapping = get_or_create_geo_mapping(df)\n",
    "    median_lat = np.median([val[0] for val in geo_mapping.values() if val and val[0] is not None])\n",
    "    median_lon = np.median([val[1] for val in geo_mapping.values() if val and val[1] is not None])\n",
    "\n",
    "    df[['Latitude', 'Longitude']] = df['Location'].apply(\n",
    "        lambda x: pd.Series(geo_mapping.get(str(x), (median_lat, median_lon)))\n",
    "    )\n",
    "\n",
    "    print(\"\\n‚öôÔ∏è Starting data cleaning process...\")\n",
    "    def clean_price(price):\n",
    "        if not isinstance(price, str): return np.nan\n",
    "        price_str = price.lower()\n",
    "        try:\n",
    "            numbers = re.findall(r'[\\d\\.]+', price_str)\n",
    "            if not numbers: return np.nan\n",
    "            value = float(numbers[0])\n",
    "            if 'cr' in price_str: return value * 100\n",
    "            return value\n",
    "        except (ValueError, IndexError): return np.nan\n",
    "\n",
    "    def clean_area(area):\n",
    "        if not isinstance(area, str): return np.nan\n",
    "        area_str = area.lower()\n",
    "        try:\n",
    "            numbers = re.findall(r'[\\d\\.]+', area_str)\n",
    "            if not numbers: return np.nan\n",
    "            value = float(numbers[0])\n",
    "            if 'yard' in area_str: return value * 9\n",
    "            return value\n",
    "        except (ValueError, IndexError): return np.nan\n",
    "\n",
    "    def clean_floor(floor):\n",
    "        if not isinstance(floor, str): return np.nan\n",
    "        floor_str = floor.lower().replace('g', '0')\n",
    "        try:\n",
    "            numbers = re.findall(r'\\d+', floor_str)\n",
    "            if numbers: return int(numbers[0])\n",
    "            return np.nan\n",
    "        except (ValueError, IndexError): return np.nan\n",
    "\n",
    "    def clean_age(age):\n",
    "        if not isinstance(age, str): return np.nan\n",
    "        age_str = age.lower()\n",
    "        if 'new' in age_str or 'under' in age_str: return 0\n",
    "        try:\n",
    "            numbers = [int(s) for s in re.findall(r'\\d+', age_str)]\n",
    "            if numbers: return sum(numbers) / len(numbers)\n",
    "            return np.nan\n",
    "        except (ValueError, IndexError): return np.nan\n",
    "\n",
    "    df.replace('-', np.nan, inplace=True)\n",
    "    df['Property-Price'] = df['Property-Price'].apply(clean_price)\n",
    "    df['Super-Built-up-Construction-Area'] = df['Super-Built-up-Construction-Area'].apply(clean_area)\n",
    "    df['Carpet-Construction-Area'] = df['Carpet-Construction-Area'].apply(clean_area)\n",
    "    df['Property-On-Floor'] = df['Property-On-Floor'].apply(clean_floor)\n",
    "    df['Age-Of-Property'] = df['Age-Of-Property'].apply(clean_age)\n",
    "    df['BHK'] = pd.to_numeric(df['BHK'].astype(str).str.extract(r'(\\d+)', expand=False), errors='coerce')\n",
    "    df['Bathroom'] = pd.to_numeric(df['Bathroom'], errors='coerce')\n",
    "    df['No-Of-Lift-Per-Block'] = pd.to_numeric(df['No-Of-Lift-Per-Block'], errors='coerce')\n",
    "    df.loc[df['Bathroom'] > 20, 'Bathroom'] = np.nan\n",
    "\n",
    "    # Add derived feature: Price per square foot (using Carpet area, fallback to median if zero)\n",
    "    df['Price_Per_SqFt'] = df['Property-Price'] / df['Carpet-Construction-Area'].clip(lower=1)\n",
    "\n",
    "    numerical_cols = ['Property-Price', 'Super-Built-up-Construction-Area', 'Carpet-Construction-Area',\n",
    "                      'Property-On-Floor', 'Age-Of-Property', 'BHK', 'Bathroom', 'No-Of-Lift-Per-Block',\n",
    "                      'Latitude', 'Longitude', 'Price_Per_SqFt']\n",
    "    for col in numerical_cols:\n",
    "        df[col].fillna(df[col].median(), inplace=True)\n",
    "\n",
    "    categorical_cols = ['City1', 'Property-Facing', 'Furniture-Details', 'Property-Status',\n",
    "                        'Current-Status', 'Parking-Details', 'Calculated_Status']\n",
    "    for col in categorical_cols:\n",
    "        df[col] = df[col].astype(str)\n",
    "        df[col].fillna(df[col].mode()[0], inplace=True)\n",
    "\n",
    "    print(\"‚úÖ Data cleaning and imputation complete.\")\n",
    "    \n",
    "    # Create Preprocessor for use in later steps\n",
    "    numerical_features = [col for col in numerical_cols]\n",
    "    # We exclude our new Calculated_Status from the training features\n",
    "    categorical_features = [col for col in categorical_cols if col != 'Calculated_Status']\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', MinMaxScaler(), numerical_features),\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "        ],\n",
    "        remainder='drop'\n",
    "    )\n",
    "    \n",
    "    return df, numerical_cols, categorical_cols, preprocessor\n",
    "\n",
    "    \n",
    "# --- Block 2: Exploratory Data Analysis (EDA) ---\n",
    "def perform_eda(df_cleaned):\n",
    "    print(\"\\n\\n--- üî¨ Starting Exploratory Data Analysis ---\")\n",
    "    print(\"\\n--- üìä Descriptive Statistics ---\")\n",
    "    print(df_cleaned.describe())\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    print(\"\\n--- üìà Visualizing Numerical Distributions ---\")\n",
    "    df_cleaned.hist(bins=30, figsize=(20, 15), layout=(4, 3))\n",
    "    plt.suptitle('Distribution of Numerical Features', size=20, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Block 3- Training the model\n",
    "def train_autoencoder(df_cleaned, preprocessor):\n",
    "    \"\"\"\n",
    "    Builds and trains the tuned autoencoder model using the provided preprocessor.\n",
    "    Added early stopping for better training efficiency.\n",
    "    \"\"\"\n",
    "    print(\"\\n\\n--- ü§ñ Preparing data for the model ---\")\n",
    "    \n",
    "    # Use the preprocessor that was created in the data pipeline\n",
    "    X_processed = preprocessor.fit_transform(df_cleaned)\n",
    "    print(f\"‚úÖ Preprocessing complete. Shape of model input data: {X_processed.shape}\")\n",
    "\n",
    "    input_dim = X_processed.shape[1]\n",
    "    embedding_dim = 64\n",
    "\n",
    "    # --- Model Architecture ---\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    encoder = Dense(256)(input_layer)\n",
    "    encoder = LeakyReLU()(encoder)\n",
    "    encoder = Dense(128)(encoder)\n",
    "    encoder = LeakyReLU()(encoder)\n",
    "    \n",
    "    bottleneck = Dense(embedding_dim)(encoder)\n",
    "    bottleneck = LeakyReLU()(bottleneck)\n",
    "\n",
    "    decoder = Dense(128)(bottleneck)\n",
    "    decoder = LeakyReLU()(decoder)\n",
    "    decoder = Dense(256)(decoder)\n",
    "    decoder = LeakyReLU()(decoder)\n",
    "    output_layer = Dense(input_dim, activation='sigmoid')(decoder)\n",
    "\n",
    "    autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "    encoder_model = Model(inputs=input_layer, outputs=bottleneck)\n",
    "    autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    print(\"\\n‚úÖ Tuned autoencoder model built.\")\n",
    "    autoencoder.summary()\n",
    "\n",
    "    print(\"\\n--- üèãÔ∏è‚Äç‚ôÇÔ∏è Training the Autoencoder ---\")\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    autoencoder.fit(\n",
    "        X_processed, X_processed,\n",
    "        epochs=200,  # Increased max epochs, but early stopping will prevent overfitting\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "        validation_split=0.1,\n",
    "        verbose=1,\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    print(\"‚úÖ Model training complete.\")\n",
    "    return encoder_model, autoencoder, X_processed\n",
    "    \n",
    "# BLOCK - 4 \n",
    "def find_similar_properties(encoder_model, autoencoder, X_processed, df_cleaned):\n",
    "    \"\"\"\n",
    "    Uses the trained encoder to find and display similar properties.\n",
    "    Reversed the query/database to match business goal: query active properties (receiving leads) to find similar expired properties.\n",
    "    Added geographic distance filter using haversine for better location-aware matching.\n",
    "    Added sorting of filtered matches by similarity score.\n",
    "    Returns the embeddings for further analysis.\n",
    "    \"\"\"\n",
    "    print(\"\\n\\n--- üéØ Finding Similar Properties ---\")\n",
    "    \n",
    "    loss = autoencoder.evaluate(X_processed, X_processed, verbose=0)\n",
    "    print(f\"\\n--- üìà Model Evaluation ---\")\n",
    "    print(f\"Final Mean Squared Error (Reconstruction Loss): {loss:.6f}\")\n",
    "    \n",
    "    all_embeddings = encoder_model.predict(X_processed)\n",
    "\n",
    "    query_mask = df_cleaned['Calculated_Status'] == 'Active'  # Queries are active properties (receiving leads)\n",
    "    database_mask = df_cleaned['Calculated_Status'] == 'Expired'  # Database is expired properties to match to\n",
    "\n",
    "    query_indices = df_cleaned[query_mask].index\n",
    "    database_indices = df_cleaned[database_mask].index\n",
    "    \n",
    "    database_embeddings = all_embeddings[database_indices]\n",
    "\n",
    "    print(f\"\\nFound {len(database_indices)} expired properties to match against.\")\n",
    "    print(f\"Found {len(query_indices)} active properties to find matches for.\")\n",
    "\n",
    "    if len(database_indices) > 0 and len(query_indices) > 0:\n",
    "        K = 10 \n",
    "        knn = NearestNeighbors(n_neighbors=K, metric='cosine')\n",
    "        knn.fit(database_embeddings)\n",
    "\n",
    "        query_original_index = np.random.choice(query_indices)\n",
    "        query_embedding = all_embeddings[query_original_index].reshape(1, -1)\n",
    "\n",
    "        print(\"\\n--- Example Match ---\")\n",
    "        print(f\"Finding similar properties for ACTIVE property at index: {query_original_index}\")\n",
    "        print(\"Query Property Details:\")\n",
    "        query_property = df_cleaned.iloc[query_original_index]\n",
    "        print(query_property)\n",
    "        print(\"-\" * 20)\n",
    "\n",
    "        distances, indices = knn.kneighbors(query_embedding)\n",
    "\n",
    "        print(f\"Top {K} initial matches found. Filtering for price and geographic relevance...\")\n",
    "        \n",
    "        query_price = query_property['Property-Price']\n",
    "        price_tolerance = 0.30\n",
    "        lower_bound = query_price * (1 - price_tolerance)\n",
    "        upper_bound = query_price * (1 + price_tolerance)\n",
    "        \n",
    "        query_lat = query_property['Latitude']\n",
    "        query_lon = query_property['Longitude']\n",
    "        max_distance_km = 5.0  # Maximum allowed distance in km for a match\n",
    "        \n",
    "        filtered_matches = []\n",
    "        for i, index in enumerate(indices[0]):\n",
    "            original_df_index = database_indices[index]\n",
    "            match_price = df_cleaned.iloc[original_df_index]['Property-Price']\n",
    "            match_lat = df_cleaned.iloc[original_df_index]['Latitude']\n",
    "            match_lon = df_cleaned.iloc[original_df_index]['Longitude']\n",
    "            dist_km = vincenty(query_lat, query_lon, match_lat, match_lon)\n",
    "            \n",
    "            if lower_bound <= match_price <= upper_bound and dist_km <= max_distance_km:\n",
    "                similarity_score = 1 - distances[0][i]\n",
    "                filtered_matches.append((original_df_index, similarity_score, dist_km))\n",
    "\n",
    "        # Sort filtered matches by similarity score descending\n",
    "        filtered_matches.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        print(f\"\\n‚úÖ Found {len(filtered_matches)} matches within {price_tolerance*100}% price range and {max_distance_km} km.\")\n",
    "        print(f\"--- Top {min(5, len(filtered_matches))} Financially and Geographically Relevant EXPIRED Properties ---\")\n",
    "        \n",
    "        if not filtered_matches:\n",
    "            print(\"No expired properties found within the price and distance constraints.\")\n",
    "        else:\n",
    "            for i, (original_df_index, similarity_score, dist_km) in enumerate(filtered_matches[:5]):\n",
    "                print(f\"\\nRank {i+1}: Property at index {original_df_index} (Similarity: {similarity_score:.4f}, Distance: {dist_km:.2f} km)\")\n",
    "                print(df_cleaned.iloc[original_df_index])\n",
    "    else:\n",
    "        print(\"\\nCould not perform matching: no active or expired properties found to process.\")\n",
    "        \n",
    "    # --- ADD THIS LINE ---\n",
    "    return all_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2d5f3621-7fd5-481b-9541-3d8ea956789f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully loaded PropertyData.csv\n",
      "‚úÖ Selected 17 features for modeling.\n",
      "\n",
      "üóìÔ∏è Calculating property status (Active/Expired/Sold)...\n",
      "‚úÖ Status calculation complete. Status distribution:\n",
      "Calculated_Status\n",
      "Expired    1055\n",
      "Active      720\n",
      "Sold         85\n",
      "Name: count, dtype: int64\n",
      "‚úÖ Found existing geocoding file at 'geo_mapping.json'. Loading...\n",
      "\n",
      "‚öôÔ∏è Starting data cleaning process...\n",
      "‚úÖ Data cleaning and imputation complete.\n"
     ]
    }
   ],
   "source": [
    "df_cleaned, numerical_cols, categorical_cols, preprocessor = run_data_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "731e35bc-e996-4deb-9e41-0ee6e0b5f8c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- ü§ñ Preparing data for the model ---\n",
      "‚úÖ Preprocessing complete. Shape of model input data: (1860, 46)\n",
      "\n",
      "‚úÖ Tuned autoencoder model built.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_6\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_6\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
       "‚îÉ<span style=\"font-weight: bold\"> Layer (type)                         </span>‚îÉ<span style=\"font-weight: bold\"> Output Shape                </span>‚îÉ<span style=\"font-weight: bold\">         Param # </span>‚îÉ\n",
       "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
       "‚îÇ input_layer_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)           ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">46</span>)                  ‚îÇ               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 ‚îÇ          <span style=\"color: #00af00; text-decoration-color: #00af00\">12,032</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ leaky_re_lu_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)           ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 ‚îÇ               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 ‚îÇ          <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ leaky_re_lu_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)           ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 ‚îÇ               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  ‚îÇ           <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ leaky_re_lu_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)           ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  ‚îÇ               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 ‚îÇ           <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ leaky_re_lu_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)           ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 ‚îÇ               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 ‚îÇ          <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ leaky_re_lu_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)           ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 ‚îÇ               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">46</span>)                  ‚îÇ          <span style=\"color: #00af00; text-decoration-color: #00af00\">11,822</span> ‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
       "‚îÉ\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m‚îÉ\n",
       "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
       "‚îÇ input_layer_3 (\u001b[38;5;33mInputLayer\u001b[0m)           ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m46\u001b[0m)                  ‚îÇ               \u001b[38;5;34m0\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense_18 (\u001b[38;5;33mDense\u001b[0m)                     ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 ‚îÇ          \u001b[38;5;34m12,032\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ leaky_re_lu_22 (\u001b[38;5;33mLeakyReLU\u001b[0m)           ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 ‚îÇ               \u001b[38;5;34m0\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense_19 (\u001b[38;5;33mDense\u001b[0m)                     ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 ‚îÇ          \u001b[38;5;34m32,896\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ leaky_re_lu_23 (\u001b[38;5;33mLeakyReLU\u001b[0m)           ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 ‚îÇ               \u001b[38;5;34m0\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense_20 (\u001b[38;5;33mDense\u001b[0m)                     ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  ‚îÇ           \u001b[38;5;34m8,256\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ leaky_re_lu_24 (\u001b[38;5;33mLeakyReLU\u001b[0m)           ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  ‚îÇ               \u001b[38;5;34m0\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense_21 (\u001b[38;5;33mDense\u001b[0m)                     ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 ‚îÇ           \u001b[38;5;34m8,320\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ leaky_re_lu_25 (\u001b[38;5;33mLeakyReLU\u001b[0m)           ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 ‚îÇ               \u001b[38;5;34m0\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense_22 (\u001b[38;5;33mDense\u001b[0m)                     ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 ‚îÇ          \u001b[38;5;34m33,024\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ leaky_re_lu_26 (\u001b[38;5;33mLeakyReLU\u001b[0m)           ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 ‚îÇ               \u001b[38;5;34m0\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense_23 (\u001b[38;5;33mDense\u001b[0m)                     ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m46\u001b[0m)                  ‚îÇ          \u001b[38;5;34m11,822\u001b[0m ‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">106,350</span> (415.43 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m106,350\u001b[0m (415.43 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">106,350</span> (415.43 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m106,350\u001b[0m (415.43 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- üèãÔ∏è‚Äç‚ôÇÔ∏è Training the Autoencoder ---\n",
      "Epoch 1/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.1183 - val_loss: 0.0368\n",
      "Epoch 2/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0283 - val_loss: 0.0227\n",
      "Epoch 3/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0189 - val_loss: 0.0179\n",
      "Epoch 4/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0146 - val_loss: 0.0123\n",
      "Epoch 5/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0094 - val_loss: 0.0103\n",
      "Epoch 6/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0075 - val_loss: 0.0084\n",
      "Epoch 7/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0062 - val_loss: 0.0062\n",
      "Epoch 8/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0049 - val_loss: 0.0052\n",
      "Epoch 9/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0043 - val_loss: 0.0048\n",
      "Epoch 10/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0035 - val_loss: 0.0040\n",
      "Epoch 11/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0031 - val_loss: 0.0040\n",
      "Epoch 12/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0037 - val_loss: 0.0038\n",
      "Epoch 13/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0035 - val_loss: 0.0041\n",
      "Epoch 14/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0031 - val_loss: 0.0037\n",
      "Epoch 15/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0035 - val_loss: 0.0034\n",
      "Epoch 16/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0032 - val_loss: 0.0035\n",
      "Epoch 17/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0034 - val_loss: 0.0036\n",
      "Epoch 18/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0030 - val_loss: 0.0032\n",
      "Epoch 19/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 20/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0024 - val_loss: 0.0032\n",
      "Epoch 21/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 22/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0026 - val_loss: 0.0030\n",
      "Epoch 23/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0024 - val_loss: 0.0025\n",
      "Epoch 24/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0024 - val_loss: 0.0020\n",
      "Epoch 25/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 26/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0019 - val_loss: 0.0021\n",
      "Epoch 27/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0022 - val_loss: 0.0020\n",
      "Epoch 28/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0021 - val_loss: 0.0019\n",
      "Epoch 29/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0015 - val_loss: 0.0018\n",
      "Epoch 30/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0012 - val_loss: 0.0021\n",
      "Epoch 31/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0016 - val_loss: 0.0018\n",
      "Epoch 32/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0016 - val_loss: 0.0019\n",
      "Epoch 33/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 34/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 35/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 36/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 37/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0010 - val_loss: 9.7508e-04\n",
      "Epoch 38/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 7.4859e-04 - val_loss: 0.0010\n",
      "Epoch 39/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.0874e-04 - val_loss: 0.0010\n",
      "Epoch 40/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.7197e-04 - val_loss: 8.3069e-04\n",
      "Epoch 41/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.3847e-04 - val_loss: 8.2315e-04\n",
      "Epoch 42/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0011 - val_loss: 8.1376e-04\n",
      "Epoch 43/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.1455e-04 - val_loss: 8.5103e-04\n",
      "Epoch 44/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.1366e-04 - val_loss: 0.0010\n",
      "Epoch 45/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 46/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0012 - val_loss: 9.2719e-04\n",
      "Epoch 47/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7051e-04 - val_loss: 8.3546e-04\n",
      "Epoch 48/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.2282e-04 - val_loss: 8.2899e-04\n",
      "Epoch 49/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.9625e-04 - val_loss: 0.0010\n",
      "Epoch 50/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.6285e-04 - val_loss: 8.2304e-04\n",
      "Epoch 51/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.3853e-04 - val_loss: 9.4680e-04\n",
      "Epoch 52/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.2724e-04 - val_loss: 7.3999e-04\n",
      "Epoch 53/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.3672e-04 - val_loss: 6.8480e-04\n",
      "Epoch 54/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.5969e-04 - val_loss: 6.8067e-04\n",
      "Epoch 55/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5.9859e-04 - val_loss: 6.9205e-04\n",
      "Epoch 56/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.6904e-04 - val_loss: 6.4535e-04\n",
      "Epoch 57/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.9738e-04 - val_loss: 6.8295e-04\n",
      "Epoch 58/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.4514e-04 - val_loss: 6.4537e-04\n",
      "Epoch 59/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.9743e-04 - val_loss: 6.3138e-04\n",
      "Epoch 60/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.7193e-04 - val_loss: 6.1805e-04\n",
      "Epoch 61/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.5349e-04 - val_loss: 6.2103e-04\n",
      "Epoch 62/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.6677e-04 - val_loss: 6.2601e-04\n",
      "Epoch 63/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.1608e-04 - val_loss: 6.3649e-04\n",
      "Epoch 64/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.5492e-04 - val_loss: 6.0795e-04\n",
      "Epoch 65/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.2959e-04 - val_loss: 7.2128e-04\n",
      "Epoch 66/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.9070e-04 - val_loss: 7.9843e-04\n",
      "Epoch 67/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.0127e-04 - val_loss: 9.8782e-04\n",
      "Epoch 68/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 9.2875e-04 - val_loss: 0.0013\n",
      "Epoch 69/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.2851e-04 - val_loss: 0.0011\n",
      "Epoch 70/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.0951e-04 - val_loss: 0.0017\n",
      "Epoch 71/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.0277e-04 - val_loss: 0.0016\n",
      "Epoch 72/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.6989e-04 - val_loss: 0.0014\n",
      "Epoch 73/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 74/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.9751e-04 - val_loss: 0.0011\n",
      "‚úÖ Model training complete.\n"
     ]
    }
   ],
   "source": [
    "# This call will now work correctly\n",
    "encoder_model, autoencoder, X_processed = train_autoencoder(df_cleaned, preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "76fdd755-6fd7-40e5-b431-7087e6dca80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- üéØ Finding Similar Properties ---\n",
      "\n",
      "--- üìà Model Evaluation ---\n",
      "Final Mean Squared Error (Reconstruction Loss): 0.000509\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "\n",
      "Found 955 expired properties to match against.\n",
      "Found 820 active properties to find matches for.\n",
      "\n",
      "--- Example Match ---\n",
      "Finding similar properties for ACTIVE property at index: 477\n",
      "Query Property Details:\n",
      "BHK                                                         3.0\n",
      "Property-Price                                             58.0\n",
      "City1                                                 Ahmedabad\n",
      "Property-On-Floor                                           6.0\n",
      "Property-Facing                                            East\n",
      "Age-Of-Property                                             2.0\n",
      "Super-Built-up-Construction-Area                          768.0\n",
      "Carpet-Construction-Area                                  461.0\n",
      "Bathroom                                                    1.0\n",
      "Furniture-Details                                   Unfurnished\n",
      "Property-Status                                   Live-Property\n",
      "Current-Status                                  Tenant-Occupied\n",
      "Location                                                 A-Sola\n",
      "Parking-Details                               Reserved-Basement\n",
      "No-Of-Lift-Per-Block                                        4.0\n",
      "Service_Expiry_Date                         2025-09-09 00:00:00\n",
      "Tag                                 613-elite-magnum-sola-may25\n",
      "Calculated_Status                                        Active\n",
      "Latitude                                                23.0757\n",
      "Longitude                                                72.509\n",
      "Price_Per_SqFt                                         0.125813\n",
      "Name: 477, dtype: object\n",
      "--------------------\n",
      "Top 10 initial matches found. Filtering for price and geographic relevance...\n",
      "\n",
      "‚úÖ Found 1 matches within 30.0% price range and 5.0 km.\n",
      "--- Top 1 Financially and Geographically Relevant EXPIRED Properties ---\n",
      "\n",
      "Rank 1: Property at index 21 (Similarity: 0.9848, Distance: 4.31 km)\n",
      "BHK                                                          3.0\n",
      "Property-Price                                              48.0\n",
      "City1                                                  Ahmedabad\n",
      "Property-On-Floor                                            1.0\n",
      "Property-Facing                                             East\n",
      "Age-Of-Property                                             10.0\n",
      "Super-Built-up-Construction-Area                           200.0\n",
      "Carpet-Construction-Area                                   170.0\n",
      "Bathroom                                                     1.0\n",
      "Furniture-Details                                    Unfurnished\n",
      "Property-Status                                    Live-Property\n",
      "Current-Status                                   Tenant-Occupied\n",
      "Location                                                  A-Gota\n",
      "Parking-Details                                Reserved-Basement\n",
      "No-Of-Lift-Per-Block                                         2.0\n",
      "Service_Expiry_Date                          2025-07-07 00:00:00\n",
      "Tag                                 10-apeksha-avenue-gota-may25\n",
      "Calculated_Status                                        Expired\n",
      "Latitude                                                 23.1013\n",
      "Longitude                                                72.5407\n",
      "Price_Per_SqFt                                          0.282353\n",
      "Name: 21, dtype: object\n"
     ]
    }
   ],
   "source": [
    "if 'encoder_model' in locals():\n",
    "    # This now passes the 'autoencoder' variable correctly\n",
    "    find_similar_properties(encoder_model, autoencoder, X_processed, df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a20177-6ff9-4288-94fb-0a31392d9c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "from geopy.geocoders import Nominatim\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import KMeans\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, LeakyReLU\n",
    "from keras.callbacks import EarlyStopping\n",
    "import math\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"\n",
    "    Calculate the great-circle distance between two points on Earth using the Haversine formula.\n",
    "    Parameters:\n",
    "        lat1, lon1: Latitude and longitude of the first point (in degrees).\n",
    "        lat2, lon2: Latitude and longitude of the second point (in degrees).\n",
    "    Returns:\n",
    "        Distance in kilometers.\n",
    "    \"\"\"\n",
    "    # Convert degrees to radians\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    \n",
    "    # Haversine formula\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat / 2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    R = 6371  # Earth's radius in km\n",
    "    return c * R\n",
    "\n",
    "def vincenty(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"\n",
    "    Calculate the geodesic distance between two points on the Earth using Vincenty's formula (in km).\n",
    "    More accurate than Haversine as it accounts for Earth's ellipsoidal shape.\n",
    "    \"\"\"\n",
    "    # WGS-84 ellipsoid parameters\n",
    "    a = 6378137.0  # semi-major axis in meters\n",
    "    f = 1 / 298.257223563  # flattening\n",
    "    b = a * (1 - f)  # semi-minor axis\n",
    "\n",
    "    # Convert degrees to radians\n",
    "    phi1 = math.radians(lat1)\n",
    "    lambda1 = math.radians(lon1)\n",
    "    phi2 = math.radians(lat2)\n",
    "    lambda2 = math.radians(lon2)\n",
    "\n",
    "    # Reduced latitudes\n",
    "    U1 = math.atan((1 - f) * math.tan(phi1))\n",
    "    U2 = math.atan((1 - f) * math.tan(phi2))\n",
    "\n",
    "    L = lambda2 - lambda1  # longitude difference\n",
    "    lam = L  # initial approximation of lambda\n",
    "\n",
    "    sinU1 = math.sin(U1)\n",
    "    cosU1 = math.cos(U1)\n",
    "    sinU2 = math.sin(U2)\n",
    "    cosU2 = math.cos(U2)\n",
    "\n",
    "    iter_limit = 100\n",
    "    for _ in range(iter_limit):\n",
    "        sin_lam = math.sin(lam)\n",
    "        cos_lam = math.cos(lam)\n",
    "\n",
    "        sin_sigma = math.sqrt((cosU2 * sin_lam)**2 + (cosU1 * sinU2 - sinU1 * cosU2 * cos_lam)**2)\n",
    "        if sin_sigma == 0:\n",
    "            return 0.0  # coincident points\n",
    "\n",
    "        cos_sigma = sinU1 * sinU2 + cosU1 * cosU2 * cos_lam\n",
    "        sigma = math.atan2(sin_sigma, cos_sigma)\n",
    "\n",
    "        sin_alpha = (cosU1 * cosU2 * sin_lam) / sin_sigma\n",
    "        cos_sq_alpha = 1 - sin_alpha**2\n",
    "\n",
    "        if cos_sq_alpha == 0:\n",
    "            return (a * math.pi) / 1000  # antipodal points, approximate\n",
    "\n",
    "        cos_2sigma_m = cos_sigma - (2 * sinU1 * sinU2) / cos_sq_alpha\n",
    "\n",
    "        C = f / 16 * cos_sq_alpha * (4 + f * (4 - 3 * cos_sq_alpha))\n",
    "\n",
    "        lam_prev = lam\n",
    "        lam = L + (1 - C) * f * sin_alpha * (sigma + C * sin_sigma * (cos_2sigma_m + C * cos_sigma * (-1 + 2 * cos_2sigma_m**2)))\n",
    "\n",
    "        if abs(lam - lam_prev) < 1e-12:\n",
    "            break\n",
    "    else:\n",
    "        raise ValueError(\"Vincenty formula failed to converge after 100 iterations\")\n",
    "\n",
    "    u_sq = cos_sq_alpha * (a**2 - b**2) / (b**2)\n",
    "    A = 1 + u_sq / 16384 * (4096 + u_sq * (-768 + u_sq * (320 - 175 * u_sq)))\n",
    "    B = u_sq / 1024 * (256 + u_sq * (-128 + u_sq * (74 - 47 * u_sq)))\n",
    "\n",
    "    delta_sigma = B * sin_sigma * (cos_2sigma_m + B / 4 * (cos_sigma * (-1 + 2 * cos_2sigma_m**2) - B / 6 * cos_2sigma_m * (-3 + 4 * sin_sigma**2) * (-3 + 4 * cos_2sigma_m**2)))\n",
    "\n",
    "    s = b * A * (sigma - delta_sigma) / 1000  # distance in km\n",
    "\n",
    "    return s\n",
    "\n",
    "def get_or_create_geo_mapping(df):\n",
    "    \"\"\"\n",
    "    Loads geocoding from a file or creates it by calling an API.\n",
    "    \"\"\"\n",
    "    json_path = 'geo_mapping.json'\n",
    "    if os.path.exists(json_path):\n",
    "        print(f\"‚úÖ Found existing geocoding file at '{json_path}'. Loading...\")\n",
    "        with open(json_path, 'r') as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    print(f\"‚ö†Ô∏è No geocoding file found. Creating a new one using geopy...\")\n",
    "    print(\"This will take several minutes as it respects API rate limits.\")\n",
    "\n",
    "    geolocator = Nominatim(user_agent=\"property_matcher_app_v6\")\n",
    "    unique_locations = df['Location'].dropna().unique()\n",
    "    geo_mapping = {}\n",
    "\n",
    "    city_prefix_map = {\n",
    "        'A': 'Ahmedabad', 'P': 'Pune', 'G': 'Gandhinagar',\n",
    "        'S': 'Surat', 'V': 'Vadodara', 'B': 'Vadodara'\n",
    "    }\n",
    "\n",
    "    for location_str in unique_locations:\n",
    "        if not isinstance(location_str, str) or '-' not in location_str:\n",
    "            continue\n",
    "\n",
    "        parts = location_str.split('-', 1)\n",
    "        prefix = parts[0].strip()\n",
    "        area = parts[1].strip()\n",
    "        city = city_prefix_map.get(prefix, 'Ahmedabad')\n",
    "\n",
    "        query = f\"{area}, {city}, India\"\n",
    "        try:\n",
    "            location_data = geolocator.geocode(query, timeout=10)\n",
    "            if location_data:\n",
    "                geo_mapping[location_str] = (location_data.latitude, location_data.longitude)\n",
    "                print(f\"‚úÖ Found: {query} -> ({location_data.latitude:.4f}, {location_data.longitude:.4f})\")\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è Not Found: {query}\")\n",
    "            time.sleep(1)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error for '{query}': {e}\")\n",
    "\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(geo_mapping, f, indent=4)\n",
    "    print(f\"‚úÖ Geocoding complete. Saved to '{json_path}'.\")\n",
    "    return geo_mapping\n",
    "\n",
    "def run_data_pipeline():\n",
    "    \"\"\"\n",
    "    Loads, cleans, and preprocesses the property data, with robust location normalization.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        property_df = pd.read_csv('PropertyData.csv', low_memory=False)\n",
    "        print(\"‚úÖ Successfully loaded PropertyData.csv\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ùå Error: 'PropertyData.csv' not found.\")\n",
    "        return None, None, None, None\n",
    "\n",
    "    df = property_df.copy()\n",
    "    features_to_use = [\n",
    "        'BHK', 'Property-Price', 'City1', 'Property-On-Floor', 'Property-Facing',\n",
    "        'Age-Of-Property', 'Super-Built-up-Construction-Area', 'Carpet-Construction-Area',\n",
    "        'Bathroom', 'Furniture-Details', 'Property-Status', 'Current-Status', 'Location',\n",
    "        'Parking-Details', 'No-Of-Lift-Per-Block', 'Service_Expiry_Date', 'Tag',\n",
    "        'Property_Type', 'Residential-Property', 'Commercial-Property-Type'\n",
    "    ]\n",
    "    df = df[features_to_use]\n",
    "    print(f\"‚úÖ Selected {len(features_to_use)} features for modeling.\")\n",
    "\n",
    "    # --- NEW: Robust Location String Normalization ---\n",
    "    print(\"\\nüßπ Normalizing location strings...\")\n",
    "    def normalize_location(loc_str):\n",
    "        if not isinstance(loc_str, str):\n",
    "            return loc_str\n",
    "        # 1. Strip leading/trailing whitespace\n",
    "        cleaned_str = loc_str.strip()\n",
    "        # 2. Correct full city names to prefixes (e.g., \"Pune-\" to \"P-\")\n",
    "        city_to_prefix = {\n",
    "            'Pune-': 'P-', 'Ahmedabad-': 'A-', 'Gandhinagar-': 'G-',\n",
    "            'Surat-': 'S-', 'Vadodara-': 'V-'\n",
    "        }\n",
    "        for city, prefix in city_to_prefix.items():\n",
    "            if cleaned_str.startswith(city):\n",
    "                cleaned_str = cleaned_str.replace(city, prefix)\n",
    "        # 3. Standardize all Vadodara prefixes to 'V-' to match your JSON\n",
    "        if cleaned_str.startswith('B-'):\n",
    "            cleaned_str = cleaned_str.replace('B-', 'V-')\n",
    "        return cleaned_str\n",
    "\n",
    "    df['Location'] = df['Location'].apply(normalize_location)\n",
    "    \n",
    "    # Status Calculation\n",
    "    print(\"\\nüóìÔ∏è Calculating property status (Active/Expired/Sold)...\")\n",
    "    sold_statuses = ['Sold-CD', 'Sold-Others', 'Rented-CD']\n",
    "    current_date = pd.to_datetime('2025-08-23')\n",
    "    df['Service_Expiry_Date'] = pd.to_datetime(df['Service_Expiry_Date'], errors='coerce', dayfirst=True)\n",
    "    \n",
    "    conditions = [\n",
    "        df['Property-Status'].isin(sold_statuses),\n",
    "        df['Service_Expiry_Date'] < current_date\n",
    "    ]\n",
    "    choices = ['Sold', 'Expired']\n",
    "    df['Calculated_Status'] = np.select(conditions, choices, default='Active')\n",
    "    print(f\"‚úÖ Status calculation complete.\")\n",
    "\n",
    "    # Load manually created geo_mapping.json\n",
    "    print(\"\\nüìç Loading geo_mapping.json...\")\n",
    "    try:\n",
    "        with open('geo_mapping.json', 'r') as f:\n",
    "            geo_mapping = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ùå Error: 'geo_mapping.json' not found.\")\n",
    "        return None, None, None, None\n",
    "\n",
    "    # Validate locations AFTER normalization\n",
    "    missing_locations = set(df['Location'].dropna().unique()) - set(geo_mapping.keys())\n",
    "    if missing_locations:\n",
    "        print(f\"‚ùå Error: {len(missing_locations)} locations not in geo_mapping.json: {missing_locations}\")\n",
    "        print(\"Please update geo_mapping.json with these locations before proceeding.\")\n",
    "        return None, None, None, None\n",
    "\n",
    "    df[['Latitude', 'Longitude']] = df['Location'].apply(\n",
    "        lambda x: pd.Series(geo_mapping.get(x, (np.nan, np.nan)))\n",
    "    )\n",
    "    \n",
    "    # (The rest of the data cleaning, clustering, and preprocessing logic continues here...)\n",
    "    print(\"\\n‚öôÔ∏è Starting data cleaning process...\")\n",
    "    def clean_price(price):\n",
    "        if not isinstance(price, str): return np.nan\n",
    "        price_str = price.lower()\n",
    "        try:\n",
    "            numbers = re.findall(r'[\\d\\.]+', price_str)\n",
    "            if not numbers: return np.nan\n",
    "            value = float(numbers[0])\n",
    "            if 'cr' in price_str: return value * 100\n",
    "            return value\n",
    "        except (ValueError, IndexError): return np.nan\n",
    "\n",
    "    def clean_area(area):\n",
    "        if not isinstance(area, str): return np.nan\n",
    "        area_str = area.lower()\n",
    "        try:\n",
    "            numbers = re.findall(r'[\\d\\.]+', area_str)\n",
    "            if not numbers: return np.nan\n",
    "            value = float(numbers[0])\n",
    "            if 'yard' in area_str: return value * 9\n",
    "            return value\n",
    "        except (ValueError, IndexError): return np.nan\n",
    "\n",
    "    df.replace('-', np.nan, inplace=True)\n",
    "    df['Property-Price'] = df['Property-Price'].apply(clean_price)\n",
    "    df['Super-Built-up-Construction-Area'] = df['Super-Built-up-Construction-Area'].apply(clean_area)\n",
    "    df['Carpet-Construction-Area'] = df['Carpet-Construction-Area'].apply(clean_area)\n",
    "    \n",
    "    for col in ['BHK', 'Bathroom', 'No-Of-Lift-Per-Block', 'Property-On-Floor', 'Age-Of-Property']:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    numerical_cols = ['Property-Price', 'Super-Built-up-Construction-Area', 'Carpet-Construction-Area',\n",
    "                      'Property-On-Floor', 'Age-Of-Property', 'BHK', 'Bathroom', 'No-Of-Lift-Per-Block',\n",
    "                      'Latitude', 'Longitude']\n",
    "    for col in numerical_cols:\n",
    "        df[col].fillna(df[col].median(), inplace=True)\n",
    "\n",
    "    categorical_cols = ['City1', 'Property-Facing', 'Furniture-Details', 'Property-Status',\n",
    "                       'Current-Status', 'Parking-Details', 'Calculated_Status',\n",
    "                       'Property_Type', 'Residential-Property', 'Commercial-Property-Type']\n",
    "    for col in categorical_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str)\n",
    "            df[col].replace('nan', 'None', inplace=True)\n",
    "            df[col].fillna('None', inplace=True)\n",
    "\n",
    "    print(\"‚úÖ Data cleaning and imputation complete.\")\n",
    "\n",
    "    numerical_features = [col for col in numerical_cols if col in df.columns]\n",
    "    categorical_features = [col for col in categorical_cols if col != 'Calculated_Status' and col in df.columns]\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', MinMaxScaler(), numerical_features),\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "        ],\n",
    "        remainder='drop'\n",
    "    )\n",
    "    \n",
    "    return df, numerical_cols, categorical_cols, preprocessor\n",
    "\n",
    "\n",
    "    \n",
    "def perform_eda(df_cleaned):\n",
    "    print(\"\\n\\n--- üî¨ Starting Exploratory Data Analysis ---\")\n",
    "    print(\"\\n--- üìä Descriptive Statistics ---\")\n",
    "    print(df_cleaned.describe())\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    print(\"\\n--- üìà Visualizing Numerical Distributions ---\")\n",
    "    df_cleaned.hist(bins=30, figsize=(20, 15), layout=(4, 3))\n",
    "    plt.suptitle('Distribution of Numerical Features', size=20, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Block 3- Training the model\n",
    "def train_autoencoder(df_cleaned, preprocessor):\n",
    "    \"\"\"\n",
    "    Builds and trains the tuned autoencoder model using the provided preprocessor.\n",
    "    Added early stopping for better training efficiency.\n",
    "    \"\"\"\n",
    "    print(\"\\n\\n--- ü§ñ Preparing data for the model ---\")\n",
    "    \n",
    "    # Use the preprocessor that was created in the data pipeline\n",
    "    X_processed = preprocessor.fit_transform(df_cleaned)\n",
    "    print(f\"‚úÖ Preprocessing complete. Shape of model input data: {X_processed.shape}\")\n",
    "\n",
    "    input_dim = X_processed.shape[1]\n",
    "    embedding_dim = 64\n",
    "\n",
    "    # --- Model Architecture ---\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    encoder = Dense(256)(input_layer)\n",
    "    encoder = LeakyReLU()(encoder)\n",
    "    encoder = Dense(128)(encoder)\n",
    "    encoder = LeakyReLU()(encoder)\n",
    "    \n",
    "    bottleneck = Dense(embedding_dim)(encoder)\n",
    "    bottleneck = LeakyReLU()(bottleneck)\n",
    "\n",
    "    decoder = Dense(128)(bottleneck)\n",
    "    decoder = LeakyReLU()(decoder)\n",
    "    decoder = Dense(256)(decoder)\n",
    "    decoder = LeakyReLU()(decoder)\n",
    "    output_layer = Dense(input_dim, activation='sigmoid')(decoder)\n",
    "\n",
    "    autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "    encoder_model = Model(inputs=input_layer, outputs=bottleneck)\n",
    "    autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    print(\"\\n‚úÖ Tuned autoencoder model built.\")\n",
    "    autoencoder.summary()\n",
    "\n",
    "    print(\"\\n--- üèãÔ∏è‚Äç‚ôÇÔ∏è Training the Autoencoder ---\")\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    autoencoder.fit(\n",
    "        X_processed, X_processed,\n",
    "        epochs=200,  # Increased max epochs, but early stopping will prevent overfitting\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "        validation_split=0.1,\n",
    "        verbose=1,\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    print(\"‚úÖ Model training complete.\")\n",
    "    return encoder_model, autoencoder, X_processed\n",
    "\n",
    "\n",
    "def find_similar_properties(encoder_model, autoencoder, X_processed, df_cleaned):\n",
    "    \"\"\"\n",
    "    Creates a mapping from each active property to similar expired properties.\n",
    "    Uses Haversine formula for distance and strict geographic filter.\n",
    "    \"\"\"\n",
    "    print(\"\\n\\n--- üéØ Finding Similar Properties ---\")\n",
    "    \n",
    "    if df_cleaned['Tag'].duplicated().any():\n",
    "        raise ValueError(\"Duplicate values found in Tag column; it must be unique.\")\n",
    "    if df_cleaned['Tag'].isna().any():\n",
    "        raise ValueError(\"Null values found in Tag column; all properties must have a Tag.\")\n",
    "    \n",
    "    assert len(X_processed) == len(df_cleaned), \"Mismatch between X_processed and df_cleaned rows\"\n",
    "    \n",
    "    loss = autoencoder.evaluate(X_processed, X_processed, verbose=0)\n",
    "    print(f\"\\n--- üìà Model Evaluation ---\")\n",
    "    print(f\"Final Mean Squared Error (Reconstruction Loss): {loss:.6f}\")\n",
    "    \n",
    "    all_embeddings = encoder_model.predict(X_processed)\n",
    "    assert len(all_embeddings) == len(df_cleaned), \"Embedding row count mismatch\"\n",
    "\n",
    "    query_mask = df_cleaned['Calculated_Status'] == 'Active'\n",
    "    database_mask = df_cleaned['Calculated_Status'] == 'Expired'\n",
    "\n",
    "    query_indices = df_cleaned[query_mask].index\n",
    "    database_indices = df_cleaned[database_mask].index\n",
    "    \n",
    "    query_positions = df_cleaned.index.get_indexer(query_indices)\n",
    "    database_positions = df_cleaned.index.get_indexer(database_indices)\n",
    "    \n",
    "    active_embeddings = all_embeddings[query_positions]\n",
    "    database_embeddings = all_embeddings[database_positions]\n",
    "\n",
    "    print(f\"\\nFound {len(query_indices)} active properties to query.\")\n",
    "    print(f\"Found {len(database_indices)} expired properties to match against.\")\n",
    "\n",
    "    active_to_expired_mapping = {}\n",
    "    rejection_reasons = {}\n",
    "\n",
    "    if len(database_indices) > 0 and len(query_indices) > 0:\n",
    "        K = 20\n",
    "        min_similarity_threshold = 0.6\n",
    "        max_distance_km = 5.0\n",
    "        price_tolerance = 0.40\n",
    "        max_matches = 10\n",
    "        \n",
    "        knn = NearestNeighbors(n_neighbors=min(K, len(database_embeddings)), metric='cosine')\n",
    "        knn.fit(database_embeddings)\n",
    "\n",
    "        print(\"\\n--- Computing KNN for Active Properties ---\")\n",
    "        distances, indices = knn.kneighbors(active_embeddings)\n",
    "\n",
    "        print(\"\\n--- Creating Mapping for All Active Properties ---\")\n",
    "        for i, active_idx in enumerate(query_indices):\n",
    "            active_tag = str(df_cleaned.loc[active_idx, 'Tag'])\n",
    "            active_price = df_cleaned.loc[active_idx, 'Property-Price']\n",
    "            active_lat = df_cleaned.loc[active_idx, 'Latitude']\n",
    "            active_lon = df_cleaned.loc[active_idx, 'Longitude']\n",
    "            active_location = df_cleaned.loc[active_idx, 'Location']\n",
    "            active_property_type = df_cleaned.loc[active_idx, 'Property_Type'] if 'Property_Type' in df_cleaned.columns else None\n",
    "            \n",
    "            lower_bound = active_price * (1 - price_tolerance)\n",
    "            upper_bound = active_price * (1 + price_tolerance)\n",
    "            \n",
    "            filtered_matches = []\n",
    "            for j, candidate_idx in enumerate(indices[i]):\n",
    "                expired_position = candidate_idx\n",
    "                expired_idx = database_indices[expired_position]\n",
    "                expired_tag = str(df_cleaned.loc[expired_idx, 'Tag'])\n",
    "                expired_price = df_cleaned.loc[expired_idx, 'Property-Price']\n",
    "                expired_lat = df_cleaned.loc[expired_idx, 'Latitude']\n",
    "                expired_lon = df_cleaned.loc[expired_idx, 'Longitude']\n",
    "                expired_location = df_cleaned.loc[expired_idx, 'Location']\n",
    "                expired_property_type = df_cleaned.loc[expired_idx, 'Property_Type'] if 'Property_Type' in df_cleaned.columns else None\n",
    "                \n",
    "                dist_km = haversine(active_lat, active_lon, expired_lat, expired_lon)\n",
    "                similarity = 1 - distances[i][j]\n",
    "                \n",
    "                reasons = []\n",
    "                if similarity < min_similarity_threshold:\n",
    "                    reasons.append(f\"Similarity {similarity:.4f} < {min_similarity_threshold}\")\n",
    "                if not (lower_bound <= expired_price <= upper_bound):\n",
    "                    reasons.append(f\"Price {expired_price} outside [{lower_bound:.2f}, {upper_bound:.2f}]\")\n",
    "                if dist_km > max_distance_km:\n",
    "                    reasons.append(f\"Distance {dist_km:.2f} km ({active_location} to {expired_location}) > {max_distance_km} km\")\n",
    "                if active_property_type and expired_property_type and active_property_type != expired_property_type:\n",
    "                    reasons.append(f\"Property_Type mismatch: {active_property_type} != {expired_property_type}\")\n",
    "                \n",
    "                if reasons:\n",
    "                    rejection_reasons.setdefault(expired_tag, []).extend(reasons)\n",
    "                \n",
    "                if (lower_bound <= expired_price <= upper_bound and \n",
    "                    dist_km <= max_distance_km and \n",
    "                    similarity >= min_similarity_threshold and\n",
    "                    (not active_property_type or active_property_type == expired_property_type)):\n",
    "                    filtered_matches.append({\n",
    "                        'expired_tag': expired_tag,\n",
    "                        'expired_index': int(expired_idx),\n",
    "                        'similarity': float(similarity),\n",
    "                        'distance_km': float(dist_km),\n",
    "                        'expired_location': expired_location\n",
    "                    })\n",
    "            \n",
    "            filtered_matches.sort(key=lambda x: x['similarity'], reverse=True)\n",
    "            active_to_expired_mapping[active_tag] = filtered_matches[:max_matches]\n",
    "            \n",
    "            if i == 0:\n",
    "                print(f\"\\n--- Example Match for Active Property Tag: {active_tag} ---\")\n",
    "                print(\"Query Property Details:\")\n",
    "                print(df_cleaned.loc[active_idx])\n",
    "                print(f\"\\nFound {len(filtered_matches)} matches within {price_tolerance*100}% price range and {max_distance_km} km.\")\n",
    "                for rank, match in enumerate(filtered_matches[:max_matches], 1):\n",
    "                    print(f\"\\nRank {rank}: Expired Property Tag {match['expired_tag']} (Index: {match['expired_index']}, Similarity: {match['similarity']:.4f}, Distance: {match['distance_km']:.2f} km, Location: {match['expired_location']})\")\n",
    "                    print(df_cleaned.loc[match['expired_index']])\n",
    "    \n",
    "    else:\n",
    "        print(\"\\nCould not perform matching: no active or expired properties found.\")\n",
    "    \n",
    "    with open('active_to_expired_mapping.json', 'w') as f:\n",
    "        json.dump(active_to_expired_mapping, f, indent=4)\n",
    "    print(\"\\n‚úÖ Mapping saved to 'active_to_expired_mapping.json'\")\n",
    "\n",
    "    rejection_data = []\n",
    "    for expired_tag, reasons in rejection_reasons.items():\n",
    "        rejection_data.append({\n",
    "            'expired_tag': expired_tag,\n",
    "            'rejection_reasons': '; '.join(set(reasons))\n",
    "        })\n",
    "    rejection_df = pd.DataFrame(rejection_data)\n",
    "    rejection_df.to_csv('rejection_reasons.csv', index=False)\n",
    "    print(\"Rejection reasons for unmatched properties saved to 'rejection_reasons.csv'\")\n",
    "\n",
    "    return all_embeddings, active_to_expired_mapping\n",
    "\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df_cleaned, numerical_cols, categorical_cols, preprocessor = run_data_pipeline()\n",
    "    if df_cleaned is not None:\n",
    "        # perform_eda(df_cleaned)  # Uncomment if you want to run EDA\n",
    "        encoder_model, autoencoder, X_processed = train_autoencoder(df_cleaned, preprocessor)\n",
    "        all_embeddings, mapping = find_similar_properties(encoder_model, autoencoder, X_processed, df_cleaned)\n",
    "\n",
    "        # --- Analysis of Unmatched Expired Properties ---\n",
    "        print(\"\\n\\n--- üîç Analyzing Unmatched Expired Properties ---\")\n",
    "        \n",
    "        # Get all expired properties' Tags\n",
    "        expired_mask = df_cleaned['Calculated_Status'] == 'Expired'\n",
    "        expired_tags = set(df_cleaned[expired_mask]['Tag'].astype(str))\n",
    "        total_expired = len(expired_tags)\n",
    "        print(f\"Total Expired Properties: {total_expired}\")\n",
    "\n",
    "        # Get all matched expired Tags from the mapping\n",
    "        matched_expired_tags = set()\n",
    "        for active_tag, matches in mapping.items():\n",
    "            for match in matches:\n",
    "                matched_expired_tags.add(match['expired_tag'])\n",
    "\n",
    "        # Find unmatched expired properties\n",
    "        unmatched_expired_tags = expired_tags - matched_expired_tags\n",
    "        num_unmatched = len(unmatched_expired_tags)\n",
    "        unmatched_percentage = (num_unmatched / total_expired * 100) if total_expired > 0 else 0\n",
    "\n",
    "        print(f\"Unmatched Expired Properties: {num_unmatched}\")\n",
    "        print(f\"Percentage of Expired Properties Unmatched: {unmatched_percentage:.2f}%\")\n",
    "\n",
    "        # Save unmatched expired properties to CSV for review\n",
    "        if num_unmatched > 0:\n",
    "            unmatched_df = df_cleaned[df_cleaned['Tag'].isin(unmatched_expired_tags)][\n",
    "                ['Tag', 'Property-Price', 'Location', 'BHK', 'Carpet-Construction-Area', 'Service_Expiry_Date']\n",
    "            ]\n",
    "            unmatched_df.to_csv('unmatched_expired_properties.csv', index=False)\n",
    "            print(\"Details of unmatched expired properties saved to 'unmatched_expired_properties.csv'\")\n",
    "        else:\n",
    "            print(\"All expired properties were matched to at least one active property.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484ba618-7fab-42ef-88f8-a6b780a43ee6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "79358f67-163b-465d-b6ff-d03552fe34f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commercial-Property-Type\n",
      "-                  858\n",
      "Others               6\n",
      "Shops-Showrooms      5\n",
      "Office-Space         4\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('PropertyData.csv')\n",
    "print(df[df['Property_Type'] == 'Residential']['Commercial-Property-Type'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c46c6d-003c-4ac5-834b-bd73470695f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5fc9586e-4a81-4c31-b244-df609e8680ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kasindra: [22.8924, 72.4913]\n",
      "Vatva: [22.9664, 72.6159]\n",
      "Chandkheda: [23.1091, 72.5849]\n",
      "New Maninagar: [22.9857, 72.6432]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('geo_mapping.json', 'r') as f:\n",
    "    geo_mapping = json.load(f)\n",
    "print(\"Kasindra:\", geo_mapping.get('A-Kasindra'))\n",
    "print(\"Vatva:\", geo_mapping.get('A-Vatva'))\n",
    "print(\"Chandkheda:\", geo_mapping.get('A-Chandkheda'))\n",
    "print(\"New Maninagar:\", geo_mapping.get('A-New Maninagar'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f2cd4cb2-b329-4061-8a87-5fd8b5c7e5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"\n",
    "    Calculate the great-circle distance between two points on Earth using the Haversine formula.\n",
    "    Parameters:\n",
    "        lat1, lon1: Latitude and longitude of the first point (in degrees).\n",
    "        lat2, lon2: Latitude and longitude of the second point (in degrees).\n",
    "    Returns:\n",
    "        Distance in kilometers.\n",
    "    \"\"\"\n",
    "    # Convert degrees to radians\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    \n",
    "    # Haversine formula\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat / 2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    R = 6371  # Earth's radius in km\n",
    "    return c * R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "708a2a33-685a-44d9-b976-0405e121a130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kasindra to Vatva: 15.183162248921903 km\n",
      "Chandkheda to New Maninagar: 14.962025332545908 km\n",
      "Kasindra to Chandkheda: 25.930649912521414 km\n",
      "Vatva to New Maninagar: 3.523703832756868 km\n"
     ]
    }
   ],
   "source": [
    "print(\"Kasindra to Vatva:\", haversine(22.8924, 72.4913, 22.9664, 72.6159), \"km\")\n",
    "print(\"Chandkheda to New Maninagar:\", haversine(23.1091, 72.5849, 22.9857, 72.6432), \"km\")\n",
    "print(\"Kasindra to Chandkheda:\", haversine(22.8924, 72.4913, 23.1091, 72.5849), \"km\")\n",
    "print(\"Vatva to New Maninagar:\", haversine(22.9664, 72.6159, 22.9857, 72.6432), \"km\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9504761-c521-41dc-9c92-e1541e3f22d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
