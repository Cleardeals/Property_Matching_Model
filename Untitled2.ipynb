{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "925b351b-ae66-4b94-a66c-604596f91b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "from geopy.geocoders import Nominatim\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns  # Assuming user has seaborn; if not, remove sns.set_style\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, LeakyReLU\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def vincenty(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"\n",
    "    Calculate the geodesic distance between two points on the Earth using Vincenty's formula (in km).\n",
    "    More accurate than Haversine as it accounts for Earth's ellipsoidal shape.\n",
    "    \"\"\"\n",
    "    # WGS-84 ellipsoid parameters\n",
    "    a = 6378137.0  # semi-major axis in meters\n",
    "    f = 1 / 298.257223563  # flattening\n",
    "    b = a * (1 - f)  # semi-minor axis\n",
    "\n",
    "    # Convert degrees to radians\n",
    "    phi1 = math.radians(lat1)\n",
    "    lambda1 = math.radians(lon1)\n",
    "    phi2 = math.radians(lat2)\n",
    "    lambda2 = math.radians(lon2)\n",
    "\n",
    "    # Reduced latitudes\n",
    "    U1 = math.atan((1 - f) * math.tan(phi1))\n",
    "    U2 = math.atan((1 - f) * math.tan(phi2))\n",
    "\n",
    "    L = lambda2 - lambda1  # longitude difference\n",
    "    lam = L  # initial approximation of lambda\n",
    "\n",
    "    sinU1 = math.sin(U1)\n",
    "    cosU1 = math.cos(U1)\n",
    "    sinU2 = math.sin(U2)\n",
    "    cosU2 = math.cos(U2)\n",
    "\n",
    "    iter_limit = 100\n",
    "    for _ in range(iter_limit):\n",
    "        sin_lam = math.sin(lam)\n",
    "        cos_lam = math.cos(lam)\n",
    "\n",
    "        sin_sigma = math.sqrt((cosU2 * sin_lam)**2 + (cosU1 * sinU2 - sinU1 * cosU2 * cos_lam)**2)\n",
    "        if sin_sigma == 0:\n",
    "            return 0.0  # coincident points\n",
    "\n",
    "        cos_sigma = sinU1 * sinU2 + cosU1 * cosU2 * cos_lam\n",
    "        sigma = math.atan2(sin_sigma, cos_sigma)\n",
    "\n",
    "        sin_alpha = (cosU1 * cosU2 * sin_lam) / sin_sigma\n",
    "        cos_sq_alpha = 1 - sin_alpha**2\n",
    "\n",
    "        if cos_sq_alpha == 0:\n",
    "            return (a * math.pi) / 1000  # antipodal points, approximate\n",
    "\n",
    "        cos_2sigma_m = cos_sigma - (2 * sinU1 * sinU2) / cos_sq_alpha\n",
    "\n",
    "        C = f / 16 * cos_sq_alpha * (4 + f * (4 - 3 * cos_sq_alpha))\n",
    "\n",
    "        lam_prev = lam\n",
    "        lam = L + (1 - C) * f * sin_alpha * (sigma + C * sin_sigma * (cos_2sigma_m + C * cos_sigma * (-1 + 2 * cos_2sigma_m**2)))\n",
    "\n",
    "        if abs(lam - lam_prev) < 1e-12:\n",
    "            break\n",
    "    else:\n",
    "        raise ValueError(\"Vincenty formula failed to converge after 100 iterations\")\n",
    "\n",
    "    u_sq = cos_sq_alpha * (a**2 - b**2) / (b**2)\n",
    "    A = 1 + u_sq / 16384 * (4096 + u_sq * (-768 + u_sq * (320 - 175 * u_sq)))\n",
    "    B = u_sq / 1024 * (256 + u_sq * (-128 + u_sq * (74 - 47 * u_sq)))\n",
    "\n",
    "    delta_sigma = B * sin_sigma * (cos_2sigma_m + B / 4 * (cos_sigma * (-1 + 2 * cos_2sigma_m**2) - B / 6 * cos_2sigma_m * (-3 + 4 * sin_sigma**2) * (-3 + 4 * cos_2sigma_m**2)))\n",
    "\n",
    "    s = b * A * (sigma - delta_sigma) / 1000  # distance in km\n",
    "\n",
    "    return s\n",
    "\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"\n",
    "    Calculate the great-circle distance between two points on the Earth (in km).\n",
    "    \"\"\"\n",
    "    R = 6371.0  # Earth radius in km\n",
    "    lat1_rad = np.radians(lat1)\n",
    "    lon1_rad = np.radians(lon1)\n",
    "    lat2_rad = np.radians(lat2)\n",
    "    lon2_rad = np.radians(lon2)\n",
    "    \n",
    "    dlat = lat2_rad - lat1_rad\n",
    "    dlon = lon2_rad - lon1_rad\n",
    "    \n",
    "    a = np.sin(dlat / 2)**2 + np.cos(lat1_rad) * np.cos(lat2_rad) * np.sin(dlon / 2)**2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "    \n",
    "    return R * c\n",
    "\n",
    "def get_or_create_geo_mapping(df):\n",
    "    \"\"\"\n",
    "    Loads geocoding from a file or creates it by calling an API.\n",
    "    \"\"\"\n",
    "    json_path = 'geo_mapping.json'\n",
    "    if os.path.exists(json_path):\n",
    "        print(f\"âœ… Found existing geocoding file at '{json_path}'. Loading...\")\n",
    "        with open(json_path, 'r') as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    print(f\"âš ï¸ No geocoding file found. Creating a new one using geopy...\")\n",
    "    print(\"This will take several minutes as it respects API rate limits.\")\n",
    "\n",
    "    geolocator = Nominatim(user_agent=\"property_matcher_app_v6\")\n",
    "    unique_locations = df['Location'].dropna().unique()\n",
    "    geo_mapping = {}\n",
    "\n",
    "    city_prefix_map = {\n",
    "        'A': 'Ahmedabad', 'P': 'Pune', 'G': 'Gandhinagar',\n",
    "        'S': 'Surat', 'V': 'Vadodara', 'B': 'Vadodara'\n",
    "    }\n",
    "\n",
    "    for location_str in unique_locations:\n",
    "        if not isinstance(location_str, str) or '-' not in location_str:\n",
    "            continue\n",
    "\n",
    "        parts = location_str.split('-', 1)\n",
    "        prefix = parts[0].strip()\n",
    "        area = parts[1].strip()\n",
    "        city = city_prefix_map.get(prefix, 'Ahmedabad')\n",
    "\n",
    "        query = f\"{area}, {city}, India\"\n",
    "        try:\n",
    "            location_data = geolocator.geocode(query, timeout=10)\n",
    "            if location_data:\n",
    "                geo_mapping[location_str] = (location_data.latitude, location_data.longitude)\n",
    "                print(f\"âœ… Found: {query} -> ({location_data.latitude:.4f}, {location_data.longitude:.4f})\")\n",
    "            else:\n",
    "                print(f\"âš ï¸ Not Found: {query}\")\n",
    "            time.sleep(1)\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error for '{query}': {e}\")\n",
    "\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(geo_mapping, f, indent=4)\n",
    "    print(f\"âœ… Geocoding complete. Saved to '{json_path}'.\")\n",
    "    return geo_mapping\n",
    "\n",
    "def run_data_pipeline():\n",
    "    \"\"\"\n",
    "    Loads, cleans, and preprocesses the property data, including geocoding.\n",
    "    Uses latitude and longitude directly as numerical features for better geographic similarity.\n",
    "    Adds a derived 'Price_Per_SqFt' feature.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        property_df = pd.read_csv('PropertyData.csv', low_memory=False)\n",
    "        print(\"âœ… Successfully loaded PropertyData.csv\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"âŒ Error: 'PropertyData.csv' not found.\")\n",
    "        return None, None, None, None\n",
    "\n",
    "    df = property_df.copy()\n",
    "    # Ensure necessary columns are included\n",
    "    features_to_use = [\n",
    "        'BHK', 'Property-Price', 'City1', 'Property-On-Floor', 'Property-Facing',\n",
    "        'Age-Of-Property', 'Super-Built-up-Construction-Area', 'Carpet-Construction-Area',\n",
    "        'Bathroom', 'Furniture-Details', 'Property-Status', 'Current-Status', 'Location', 'Parking-Details',\n",
    "        'No-Of-Lift-Per-Block', 'Service_Expiry_Date', 'Tag'\n",
    "    ]\n",
    "    df = df[features_to_use]\n",
    "    print(f\"âœ… Selected {len(features_to_use)} features for modeling.\")\n",
    "\n",
    "    # --- UPDATED: Three-State Status Calculation ---\n",
    "    print(\"\\nğŸ—“ï¸ Calculating property status (Active/Expired/Sold)...\")\n",
    "    \n",
    "    # Define what constitutes a 'Sold' property\n",
    "    sold_statuses = ['Sold-CD', 'Sold-Others', 'Rented-CD']\n",
    "    \n",
    "    # Define the reference date for checking expiry (updated to current date)\n",
    "    current_date = pd.to_datetime('2025-08-23')\n",
    "    \n",
    "    # Convert Service_Expiry_Date to datetime objects\n",
    "    df['Service_Expiry_Date'] = pd.to_datetime(df['Service_Expiry_Date'], errors='coerce', dayfirst=True)\n",
    "    \n",
    "    # Create the new reliable status column with three states using np.select\n",
    "    conditions = [\n",
    "        df['Property-Status'].isin(sold_statuses),\n",
    "        df['Service_Expiry_Date'] < current_date\n",
    "    ]\n",
    "    choices = ['Sold', 'Expired']\n",
    "    df['Calculated_Status'] = np.select(conditions, choices, default='Active')\n",
    "    \n",
    "    print(f\"âœ… Status calculation complete. Status distribution:\")\n",
    "    print(df['Calculated_Status'].value_counts())\n",
    "\n",
    "    geo_mapping = get_or_create_geo_mapping(df)\n",
    "    median_lat = np.median([val[0] for val in geo_mapping.values() if val and val[0] is not None])\n",
    "    median_lon = np.median([val[1] for val in geo_mapping.values() if val and val[1] is not None])\n",
    "\n",
    "    df[['Latitude', 'Longitude']] = df['Location'].apply(\n",
    "        lambda x: pd.Series(geo_mapping.get(str(x), (median_lat, median_lon)))\n",
    "    )\n",
    "\n",
    "    print(\"\\nâš™ï¸ Starting data cleaning process...\")\n",
    "    def clean_price(price):\n",
    "        if not isinstance(price, str): return np.nan\n",
    "        price_str = price.lower()\n",
    "        try:\n",
    "            numbers = re.findall(r'[\\d\\.]+', price_str)\n",
    "            if not numbers: return np.nan\n",
    "            value = float(numbers[0])\n",
    "            if 'cr' in price_str: return value * 100\n",
    "            return value\n",
    "        except (ValueError, IndexError): return np.nan\n",
    "\n",
    "    def clean_area(area):\n",
    "        if not isinstance(area, str): return np.nan\n",
    "        area_str = area.lower()\n",
    "        try:\n",
    "            numbers = re.findall(r'[\\d\\.]+', area_str)\n",
    "            if not numbers: return np.nan\n",
    "            value = float(numbers[0])\n",
    "            if 'yard' in area_str: return value * 9\n",
    "            return value\n",
    "        except (ValueError, IndexError): return np.nan\n",
    "\n",
    "    def clean_floor(floor):\n",
    "        if not isinstance(floor, str): return np.nan\n",
    "        floor_str = floor.lower().replace('g', '0')\n",
    "        try:\n",
    "            numbers = re.findall(r'\\d+', floor_str)\n",
    "            if numbers: return int(numbers[0])\n",
    "            return np.nan\n",
    "        except (ValueError, IndexError): return np.nan\n",
    "\n",
    "    def clean_age(age):\n",
    "        if not isinstance(age, str): return np.nan\n",
    "        age_str = age.lower()\n",
    "        if 'new' in age_str or 'under' in age_str: return 0\n",
    "        try:\n",
    "            numbers = [int(s) for s in re.findall(r'\\d+', age_str)]\n",
    "            if numbers: return sum(numbers) / len(numbers)\n",
    "            return np.nan\n",
    "        except (ValueError, IndexError): return np.nan\n",
    "\n",
    "    df.replace('-', np.nan, inplace=True)\n",
    "    df['Property-Price'] = df['Property-Price'].apply(clean_price)\n",
    "    df['Super-Built-up-Construction-Area'] = df['Super-Built-up-Construction-Area'].apply(clean_area)\n",
    "    df['Carpet-Construction-Area'] = df['Carpet-Construction-Area'].apply(clean_area)\n",
    "    df['Property-On-Floor'] = df['Property-On-Floor'].apply(clean_floor)\n",
    "    df['Age-Of-Property'] = df['Age-Of-Property'].apply(clean_age)\n",
    "    df['BHK'] = pd.to_numeric(df['BHK'].astype(str).str.extract(r'(\\d+)', expand=False), errors='coerce')\n",
    "    df['Bathroom'] = pd.to_numeric(df['Bathroom'], errors='coerce')\n",
    "    df['No-Of-Lift-Per-Block'] = pd.to_numeric(df['No-Of-Lift-Per-Block'], errors='coerce')\n",
    "    df.loc[df['Bathroom'] > 20, 'Bathroom'] = np.nan\n",
    "\n",
    "    # Add derived feature: Price per square foot (using Carpet area, fallback to median if zero)\n",
    "    df['Price_Per_SqFt'] = df['Property-Price'] / df['Carpet-Construction-Area'].clip(lower=1)\n",
    "\n",
    "    numerical_cols = ['Property-Price', 'Super-Built-up-Construction-Area', 'Carpet-Construction-Area',\n",
    "                      'Property-On-Floor', 'Age-Of-Property', 'BHK', 'Bathroom', 'No-Of-Lift-Per-Block',\n",
    "                      'Latitude', 'Longitude', 'Price_Per_SqFt']\n",
    "    for col in numerical_cols:\n",
    "        df[col].fillna(df[col].median(), inplace=True)\n",
    "\n",
    "    categorical_cols = ['City1', 'Property-Facing', 'Furniture-Details', 'Property-Status',\n",
    "                        'Current-Status', 'Parking-Details', 'Calculated_Status']\n",
    "    for col in categorical_cols:\n",
    "        df[col] = df[col].astype(str)\n",
    "        df[col].fillna(df[col].mode()[0], inplace=True)\n",
    "\n",
    "    print(\"âœ… Data cleaning and imputation complete.\")\n",
    "    \n",
    "    # Create Preprocessor for use in later steps\n",
    "    numerical_features = [col for col in numerical_cols]\n",
    "    # We exclude our new Calculated_Status from the training features\n",
    "    categorical_features = [col for col in categorical_cols if col != 'Calculated_Status']\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', MinMaxScaler(), numerical_features),\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "        ],\n",
    "        remainder='drop'\n",
    "    )\n",
    "    \n",
    "    return df, numerical_cols, categorical_cols, preprocessor\n",
    "\n",
    "    \n",
    "# --- Block 2: Exploratory Data Analysis (EDA) ---\n",
    "def perform_eda(df_cleaned):\n",
    "    print(\"\\n\\n--- ğŸ”¬ Starting Exploratory Data Analysis ---\")\n",
    "    print(\"\\n--- ğŸ“Š Descriptive Statistics ---\")\n",
    "    print(df_cleaned.describe())\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    print(\"\\n--- ğŸ“ˆ Visualizing Numerical Distributions ---\")\n",
    "    df_cleaned.hist(bins=30, figsize=(20, 15), layout=(4, 3))\n",
    "    plt.suptitle('Distribution of Numerical Features', size=20, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Block 3- Training the model\n",
    "def train_autoencoder(df_cleaned, preprocessor):\n",
    "    \"\"\"\n",
    "    Builds and trains the tuned autoencoder model using the provided preprocessor.\n",
    "    Added early stopping for better training efficiency.\n",
    "    \"\"\"\n",
    "    print(\"\\n\\n--- ğŸ¤– Preparing data for the model ---\")\n",
    "    \n",
    "    # Use the preprocessor that was created in the data pipeline\n",
    "    X_processed = preprocessor.fit_transform(df_cleaned)\n",
    "    print(f\"âœ… Preprocessing complete. Shape of model input data: {X_processed.shape}\")\n",
    "\n",
    "    input_dim = X_processed.shape[1]\n",
    "    embedding_dim = 64\n",
    "\n",
    "    # --- Model Architecture ---\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    encoder = Dense(256)(input_layer)\n",
    "    encoder = LeakyReLU()(encoder)\n",
    "    encoder = Dense(128)(encoder)\n",
    "    encoder = LeakyReLU()(encoder)\n",
    "    \n",
    "    bottleneck = Dense(embedding_dim)(encoder)\n",
    "    bottleneck = LeakyReLU()(bottleneck)\n",
    "\n",
    "    decoder = Dense(128)(bottleneck)\n",
    "    decoder = LeakyReLU()(decoder)\n",
    "    decoder = Dense(256)(decoder)\n",
    "    decoder = LeakyReLU()(decoder)\n",
    "    output_layer = Dense(input_dim, activation='sigmoid')(decoder)\n",
    "\n",
    "    autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "    encoder_model = Model(inputs=input_layer, outputs=bottleneck)\n",
    "    autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    print(\"\\nâœ… Tuned autoencoder model built.\")\n",
    "    autoencoder.summary()\n",
    "\n",
    "    print(\"\\n--- ğŸ‹ï¸â€â™‚ï¸ Training the Autoencoder ---\")\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    autoencoder.fit(\n",
    "        X_processed, X_processed,\n",
    "        epochs=200,  # Increased max epochs, but early stopping will prevent overfitting\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "        validation_split=0.1,\n",
    "        verbose=1,\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    print(\"âœ… Model training complete.\")\n",
    "    return encoder_model, autoencoder, X_processed\n",
    "    \n",
    "# BLOCK - 4 \n",
    "def find_similar_properties(encoder_model, autoencoder, X_processed, df_cleaned):\n",
    "    \"\"\"\n",
    "    Uses the trained encoder to find and display similar properties.\n",
    "    Reversed the query/database to match business goal: query active properties (receiving leads) to find similar expired properties.\n",
    "    Added geographic distance filter using haversine for better location-aware matching.\n",
    "    Added sorting of filtered matches by similarity score.\n",
    "    Returns the embeddings for further analysis.\n",
    "    \"\"\"\n",
    "    print(\"\\n\\n--- ğŸ¯ Finding Similar Properties ---\")\n",
    "    \n",
    "    loss = autoencoder.evaluate(X_processed, X_processed, verbose=0)\n",
    "    print(f\"\\n--- ğŸ“ˆ Model Evaluation ---\")\n",
    "    print(f\"Final Mean Squared Error (Reconstruction Loss): {loss:.6f}\")\n",
    "    \n",
    "    all_embeddings = encoder_model.predict(X_processed)\n",
    "\n",
    "    query_mask = df_cleaned['Calculated_Status'] == 'Active'  # Queries are active properties (receiving leads)\n",
    "    database_mask = df_cleaned['Calculated_Status'] == 'Expired'  # Database is expired properties to match to\n",
    "\n",
    "    query_indices = df_cleaned[query_mask].index\n",
    "    database_indices = df_cleaned[database_mask].index\n",
    "    \n",
    "    database_embeddings = all_embeddings[database_indices]\n",
    "\n",
    "    print(f\"\\nFound {len(database_indices)} expired properties to match against.\")\n",
    "    print(f\"Found {len(query_indices)} active properties to find matches for.\")\n",
    "\n",
    "    if len(database_indices) > 0 and len(query_indices) > 0:\n",
    "        K = 10 \n",
    "        knn = NearestNeighbors(n_neighbors=K, metric='cosine')\n",
    "        knn.fit(database_embeddings)\n",
    "\n",
    "        query_original_index = np.random.choice(query_indices)\n",
    "        query_embedding = all_embeddings[query_original_index].reshape(1, -1)\n",
    "\n",
    "        print(\"\\n--- Example Match ---\")\n",
    "        print(f\"Finding similar properties for ACTIVE property at index: {query_original_index}\")\n",
    "        print(\"Query Property Details:\")\n",
    "        query_property = df_cleaned.iloc[query_original_index]\n",
    "        print(query_property)\n",
    "        print(\"-\" * 20)\n",
    "\n",
    "        distances, indices = knn.kneighbors(query_embedding)\n",
    "\n",
    "        print(f\"Top {K} initial matches found. Filtering for price and geographic relevance...\")\n",
    "        \n",
    "        query_price = query_property['Property-Price']\n",
    "        price_tolerance = 0.30\n",
    "        lower_bound = query_price * (1 - price_tolerance)\n",
    "        upper_bound = query_price * (1 + price_tolerance)\n",
    "        \n",
    "        query_lat = query_property['Latitude']\n",
    "        query_lon = query_property['Longitude']\n",
    "        max_distance_km = 5.0  # Maximum allowed distance in km for a match\n",
    "        \n",
    "        filtered_matches = []\n",
    "        for i, index in enumerate(indices[0]):\n",
    "            original_df_index = database_indices[index]\n",
    "            match_price = df_cleaned.iloc[original_df_index]['Property-Price']\n",
    "            match_lat = df_cleaned.iloc[original_df_index]['Latitude']\n",
    "            match_lon = df_cleaned.iloc[original_df_index]['Longitude']\n",
    "            dist_km = vincenty(query_lat, query_lon, match_lat, match_lon)\n",
    "            \n",
    "            if lower_bound <= match_price <= upper_bound and dist_km <= max_distance_km:\n",
    "                similarity_score = 1 - distances[0][i]\n",
    "                filtered_matches.append((original_df_index, similarity_score, dist_km))\n",
    "\n",
    "        # Sort filtered matches by similarity score descending\n",
    "        filtered_matches.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        print(f\"\\nâœ… Found {len(filtered_matches)} matches within {price_tolerance*100}% price range and {max_distance_km} km.\")\n",
    "        print(f\"--- Top {min(5, len(filtered_matches))} Financially and Geographically Relevant EXPIRED Properties ---\")\n",
    "        \n",
    "        if not filtered_matches:\n",
    "            print(\"No expired properties found within the price and distance constraints.\")\n",
    "        else:\n",
    "            for i, (original_df_index, similarity_score, dist_km) in enumerate(filtered_matches[:5]):\n",
    "                print(f\"\\nRank {i+1}: Property at index {original_df_index} (Similarity: {similarity_score:.4f}, Distance: {dist_km:.2f} km)\")\n",
    "                print(df_cleaned.iloc[original_df_index])\n",
    "    else:\n",
    "        print(\"\\nCould not perform matching: no active or expired properties found to process.\")\n",
    "        \n",
    "    # --- ADD THIS LINE ---\n",
    "    return all_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2d5f3621-7fd5-481b-9541-3d8ea956789f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Successfully loaded PropertyData.csv\n",
      "âœ… Selected 17 features for modeling.\n",
      "\n",
      "ğŸ—“ï¸ Calculating property status (Active/Expired/Sold)...\n",
      "âœ… Status calculation complete. Status distribution:\n",
      "Calculated_Status\n",
      "Expired    1055\n",
      "Active      720\n",
      "Sold         85\n",
      "Name: count, dtype: int64\n",
      "âœ… Found existing geocoding file at 'geo_mapping.json'. Loading...\n",
      "\n",
      "âš™ï¸ Starting data cleaning process...\n",
      "âœ… Data cleaning and imputation complete.\n"
     ]
    }
   ],
   "source": [
    "df_cleaned, numerical_cols, categorical_cols, preprocessor = run_data_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "731e35bc-e996-4deb-9e41-0ee6e0b5f8c2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- ğŸ¤– Preparing data for the model ---\n",
      "âœ… Preprocessing complete. Shape of model input data: (1860, 46)\n",
      "\n",
      "âœ… Tuned autoencoder model built.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_6\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_6\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                         </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape                </span>â”ƒ<span style=\"font-weight: bold\">         Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">46</span>)                  â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">12,032</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ leaky_re_lu_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ leaky_re_lu_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ leaky_re_lu_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ leaky_re_lu_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ leaky_re_lu_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">46</span>)                  â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">11,822</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer_3 (\u001b[38;5;33mInputLayer\u001b[0m)           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m46\u001b[0m)                  â”‚               \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_18 (\u001b[38;5;33mDense\u001b[0m)                     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 â”‚          \u001b[38;5;34m12,032\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ leaky_re_lu_22 (\u001b[38;5;33mLeakyReLU\u001b[0m)           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 â”‚               \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_19 (\u001b[38;5;33mDense\u001b[0m)                     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 â”‚          \u001b[38;5;34m32,896\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ leaky_re_lu_23 (\u001b[38;5;33mLeakyReLU\u001b[0m)           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 â”‚               \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_20 (\u001b[38;5;33mDense\u001b[0m)                     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  â”‚           \u001b[38;5;34m8,256\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ leaky_re_lu_24 (\u001b[38;5;33mLeakyReLU\u001b[0m)           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  â”‚               \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_21 (\u001b[38;5;33mDense\u001b[0m)                     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 â”‚           \u001b[38;5;34m8,320\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ leaky_re_lu_25 (\u001b[38;5;33mLeakyReLU\u001b[0m)           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 â”‚               \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_22 (\u001b[38;5;33mDense\u001b[0m)                     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 â”‚          \u001b[38;5;34m33,024\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ leaky_re_lu_26 (\u001b[38;5;33mLeakyReLU\u001b[0m)           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 â”‚               \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_23 (\u001b[38;5;33mDense\u001b[0m)                     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m46\u001b[0m)                  â”‚          \u001b[38;5;34m11,822\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">106,350</span> (415.43 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m106,350\u001b[0m (415.43 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">106,350</span> (415.43 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m106,350\u001b[0m (415.43 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ğŸ‹ï¸â€â™‚ï¸ Training the Autoencoder ---\n",
      "Epoch 1/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.1183 - val_loss: 0.0368\n",
      "Epoch 2/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0283 - val_loss: 0.0227\n",
      "Epoch 3/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0189 - val_loss: 0.0179\n",
      "Epoch 4/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0146 - val_loss: 0.0123\n",
      "Epoch 5/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0094 - val_loss: 0.0103\n",
      "Epoch 6/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0075 - val_loss: 0.0084\n",
      "Epoch 7/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0062 - val_loss: 0.0062\n",
      "Epoch 8/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0049 - val_loss: 0.0052\n",
      "Epoch 9/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0043 - val_loss: 0.0048\n",
      "Epoch 10/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0035 - val_loss: 0.0040\n",
      "Epoch 11/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0031 - val_loss: 0.0040\n",
      "Epoch 12/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0037 - val_loss: 0.0038\n",
      "Epoch 13/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0035 - val_loss: 0.0041\n",
      "Epoch 14/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0031 - val_loss: 0.0037\n",
      "Epoch 15/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0035 - val_loss: 0.0034\n",
      "Epoch 16/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0032 - val_loss: 0.0035\n",
      "Epoch 17/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0034 - val_loss: 0.0036\n",
      "Epoch 18/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0030 - val_loss: 0.0032\n",
      "Epoch 19/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0032 - val_loss: 0.0031\n",
      "Epoch 20/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0024 - val_loss: 0.0032\n",
      "Epoch 21/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0028 - val_loss: 0.0029\n",
      "Epoch 22/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0026 - val_loss: 0.0030\n",
      "Epoch 23/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0024 - val_loss: 0.0025\n",
      "Epoch 24/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0024 - val_loss: 0.0020\n",
      "Epoch 25/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 26/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0019 - val_loss: 0.0021\n",
      "Epoch 27/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0022 - val_loss: 0.0020\n",
      "Epoch 28/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0021 - val_loss: 0.0019\n",
      "Epoch 29/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0015 - val_loss: 0.0018\n",
      "Epoch 30/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0012 - val_loss: 0.0021\n",
      "Epoch 31/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0016 - val_loss: 0.0018\n",
      "Epoch 32/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0016 - val_loss: 0.0019\n",
      "Epoch 33/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0015 - val_loss: 0.0016\n",
      "Epoch 34/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 35/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 36/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 37/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0010 - val_loss: 9.7508e-04\n",
      "Epoch 38/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 7.4859e-04 - val_loss: 0.0010\n",
      "Epoch 39/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.0874e-04 - val_loss: 0.0010\n",
      "Epoch 40/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.7197e-04 - val_loss: 8.3069e-04\n",
      "Epoch 41/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.3847e-04 - val_loss: 8.2315e-04\n",
      "Epoch 42/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0011 - val_loss: 8.1376e-04\n",
      "Epoch 43/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.1455e-04 - val_loss: 8.5103e-04\n",
      "Epoch 44/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.1366e-04 - val_loss: 0.0010\n",
      "Epoch 45/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 46/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0012 - val_loss: 9.2719e-04\n",
      "Epoch 47/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.7051e-04 - val_loss: 8.3546e-04\n",
      "Epoch 48/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.2282e-04 - val_loss: 8.2899e-04\n",
      "Epoch 49/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.9625e-04 - val_loss: 0.0010\n",
      "Epoch 50/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6.6285e-04 - val_loss: 8.2304e-04\n",
      "Epoch 51/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.3853e-04 - val_loss: 9.4680e-04\n",
      "Epoch 52/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.2724e-04 - val_loss: 7.3999e-04\n",
      "Epoch 53/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.3672e-04 - val_loss: 6.8480e-04\n",
      "Epoch 54/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.5969e-04 - val_loss: 6.8067e-04\n",
      "Epoch 55/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5.9859e-04 - val_loss: 6.9205e-04\n",
      "Epoch 56/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.6904e-04 - val_loss: 6.4535e-04\n",
      "Epoch 57/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.9738e-04 - val_loss: 6.8295e-04\n",
      "Epoch 58/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.4514e-04 - val_loss: 6.4537e-04\n",
      "Epoch 59/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.9743e-04 - val_loss: 6.3138e-04\n",
      "Epoch 60/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.7193e-04 - val_loss: 6.1805e-04\n",
      "Epoch 61/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.5349e-04 - val_loss: 6.2103e-04\n",
      "Epoch 62/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.6677e-04 - val_loss: 6.2601e-04\n",
      "Epoch 63/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.1608e-04 - val_loss: 6.3649e-04\n",
      "Epoch 64/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.5492e-04 - val_loss: 6.0795e-04\n",
      "Epoch 65/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.2959e-04 - val_loss: 7.2128e-04\n",
      "Epoch 66/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.9070e-04 - val_loss: 7.9843e-04\n",
      "Epoch 67/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.0127e-04 - val_loss: 9.8782e-04\n",
      "Epoch 68/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 9.2875e-04 - val_loss: 0.0013\n",
      "Epoch 69/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.2851e-04 - val_loss: 0.0011\n",
      "Epoch 70/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.0951e-04 - val_loss: 0.0017\n",
      "Epoch 71/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.0277e-04 - val_loss: 0.0016\n",
      "Epoch 72/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.6989e-04 - val_loss: 0.0014\n",
      "Epoch 73/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 74/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.9751e-04 - val_loss: 0.0011\n",
      "âœ… Model training complete.\n"
     ]
    }
   ],
   "source": [
    "# This call will now work correctly\n",
    "encoder_model, autoencoder, X_processed = train_autoencoder(df_cleaned, preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "76fdd755-6fd7-40e5-b431-7087e6dca80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- ğŸ¯ Finding Similar Properties ---\n",
      "\n",
      "--- ğŸ“ˆ Model Evaluation ---\n",
      "Final Mean Squared Error (Reconstruction Loss): 0.000509\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "\n",
      "Found 955 expired properties to match against.\n",
      "Found 820 active properties to find matches for.\n",
      "\n",
      "--- Example Match ---\n",
      "Finding similar properties for ACTIVE property at index: 477\n",
      "Query Property Details:\n",
      "BHK                                                         3.0\n",
      "Property-Price                                             58.0\n",
      "City1                                                 Ahmedabad\n",
      "Property-On-Floor                                           6.0\n",
      "Property-Facing                                            East\n",
      "Age-Of-Property                                             2.0\n",
      "Super-Built-up-Construction-Area                          768.0\n",
      "Carpet-Construction-Area                                  461.0\n",
      "Bathroom                                                    1.0\n",
      "Furniture-Details                                   Unfurnished\n",
      "Property-Status                                   Live-Property\n",
      "Current-Status                                  Tenant-Occupied\n",
      "Location                                                 A-Sola\n",
      "Parking-Details                               Reserved-Basement\n",
      "No-Of-Lift-Per-Block                                        4.0\n",
      "Service_Expiry_Date                         2025-09-09 00:00:00\n",
      "Tag                                 613-elite-magnum-sola-may25\n",
      "Calculated_Status                                        Active\n",
      "Latitude                                                23.0757\n",
      "Longitude                                                72.509\n",
      "Price_Per_SqFt                                         0.125813\n",
      "Name: 477, dtype: object\n",
      "--------------------\n",
      "Top 10 initial matches found. Filtering for price and geographic relevance...\n",
      "\n",
      "âœ… Found 1 matches within 30.0% price range and 5.0 km.\n",
      "--- Top 1 Financially and Geographically Relevant EXPIRED Properties ---\n",
      "\n",
      "Rank 1: Property at index 21 (Similarity: 0.9848, Distance: 4.31 km)\n",
      "BHK                                                          3.0\n",
      "Property-Price                                              48.0\n",
      "City1                                                  Ahmedabad\n",
      "Property-On-Floor                                            1.0\n",
      "Property-Facing                                             East\n",
      "Age-Of-Property                                             10.0\n",
      "Super-Built-up-Construction-Area                           200.0\n",
      "Carpet-Construction-Area                                   170.0\n",
      "Bathroom                                                     1.0\n",
      "Furniture-Details                                    Unfurnished\n",
      "Property-Status                                    Live-Property\n",
      "Current-Status                                   Tenant-Occupied\n",
      "Location                                                  A-Gota\n",
      "Parking-Details                                Reserved-Basement\n",
      "No-Of-Lift-Per-Block                                         2.0\n",
      "Service_Expiry_Date                          2025-07-07 00:00:00\n",
      "Tag                                 10-apeksha-avenue-gota-may25\n",
      "Calculated_Status                                        Expired\n",
      "Latitude                                                 23.1013\n",
      "Longitude                                                72.5407\n",
      "Price_Per_SqFt                                          0.282353\n",
      "Name: 21, dtype: object\n"
     ]
    }
   ],
   "source": [
    "if 'encoder_model' in locals():\n",
    "    # This now passes the 'autoencoder' variable correctly\n",
    "    find_similar_properties(encoder_model, autoencoder, X_processed, df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "07a20177-6ff9-4288-94fb-0a31392d9c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Successfully loaded PropertyData.csv\n",
      "âœ… Selected 20 features for modeling.\n",
      "\n",
      "ğŸ§¹ Cleaning Location values...\n",
      "âš ï¸ Found 7 rows with Location = '-'. Setting to NaN.\n",
      "\n",
      "ğŸ§¹ Cleaning Property_Type, Residential-Property, Commercial-Property-Type...\n",
      "Inconsistent Residential properties with Commercial-Property-Type:\n",
      "Empty DataFrame\n",
      "Columns: [Tag, Property_Type, Residential-Property, Commercial-Property-Type]\n",
      "Index: []\n",
      "Inconsistent Commercial properties with Residential-Property:\n",
      "Empty DataFrame\n",
      "Columns: [Tag, Property_Type, Residential-Property, Commercial-Property-Type]\n",
      "Index: []\n",
      "\n",
      "ğŸ—“ï¸ Calculating property status (Active/Expired/Sold)...\n",
      "Null Service_Expiry_Date count: 8\n",
      "Sample Service_Expiry_Date values:\n",
      "                                        Tag Service_Expiry_Date\n",
      "0                                 recalling                 NaT\n",
      "1  001-sutariya-complex-navrangpura-march25          2025-06-06\n",
      "2      1-804-aryan-gloria-south-bopal-feb25          2025-06-07\n",
      "3            1-akshar-complex-dahegma-jan25          2025-03-18\n",
      "4               1-arvind-park-society-jun25          2025-10-26\n",
      "âœ… Status calculation complete. Status distribution:\n",
      "Calculated_Status\n",
      "Expired    1055\n",
      "Active      720\n",
      "Sold         85\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Checking for duplicate Tags...\n",
      "Found 1 duplicate Tags. Assigning unique Tags...\n",
      "Exported duplicate Tags to 'duplicate_tags.csv' for review.\n",
      "Assigned unique Tags by appending index.\n",
      "\n",
      "ğŸ“ Loading geo_mapping.json...\n",
      "Coordinate assignment complete. Sample coordinates:\n",
      "        Location  Latitude  Longitude\n",
      "0   A-Chandkheda   23.1091    72.5849\n",
      "1  A-Navrangpura   23.0365    72.5611\n",
      "2  A-South Bopal   23.0206    72.4687\n",
      "3      G-Dahegam   23.1687    72.8102\n",
      "4    A-Bapunagar   23.0387    72.6308\n",
      "\n",
      "âš™ï¸ Starting data cleaning process...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_14300\\508624454.py:316: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(df[col].median(), inplace=True)\n",
      "C:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_14300\\508624454.py:324: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].replace('nan', 'None', inplace=True)\n",
      "C:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_14300\\508624454.py:325: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna('None', inplace=True)\n",
      "C:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_14300\\508624454.py:324: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].replace('nan', 'None', inplace=True)\n",
      "C:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_14300\\508624454.py:325: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna('None', inplace=True)\n",
      "C:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_14300\\508624454.py:324: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].replace('nan', 'None', inplace=True)\n",
      "C:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_14300\\508624454.py:325: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna('None', inplace=True)\n",
      "C:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_14300\\508624454.py:324: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].replace('nan', 'None', inplace=True)\n",
      "C:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_14300\\508624454.py:325: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna('None', inplace=True)\n",
      "C:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_14300\\508624454.py:324: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].replace('nan', 'None', inplace=True)\n",
      "C:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_14300\\508624454.py:325: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna('None', inplace=True)\n",
      "C:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_14300\\508624454.py:324: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].replace('nan', 'None', inplace=True)\n",
      "C:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_14300\\508624454.py:325: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna('None', inplace=True)\n",
      "C:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_14300\\508624454.py:324: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].replace('nan', 'None', inplace=True)\n",
      "C:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_14300\\508624454.py:325: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna('None', inplace=True)\n",
      "C:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_14300\\508624454.py:324: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].replace('nan', 'None', inplace=True)\n",
      "C:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_14300\\508624454.py:325: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna('None', inplace=True)\n",
      "C:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_14300\\508624454.py:324: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].replace('nan', 'None', inplace=True)\n",
      "C:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_14300\\508624454.py:325: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna('None', inplace=True)\n",
      "C:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_14300\\508624454.py:324: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].replace('nan', 'None', inplace=True)\n",
      "C:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_14300\\508624454.py:325: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna('None', inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Data cleaning and imputation complete.\n",
      "Property_Type distribution:\n",
      "Property_Type\n",
      "Residential    1454\n",
      "Commercial      401\n",
      "None              5\n",
      "Name: count, dtype: int64\n",
      "Residential-Property distribution:\n",
      "Residential-Property\n",
      "Flat-Apartment-Tower                        965\n",
      "Row-House-Bunglows-Villa-Duplex-Tenament    439\n",
      "None                                        414\n",
      "Residential-Plot-Land                        26\n",
      "Penthouse                                    16\n",
      "Name: count, dtype: int64\n",
      "Commercial-Property-Type distribution:\n",
      "Commercial-Property-Type\n",
      "None               1465\n",
      "Shops-Showrooms     182\n",
      "Office-Space        177\n",
      "Others               36\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "--- ğŸ¤– Preparing data for the model ---\n",
      "âœ… Preprocessing complete. Shape of model input data: (1860, 58)\n",
      "\n",
      "âœ… Tuned autoencoder model built.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_24\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_24\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                         </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape                </span>â”ƒ<span style=\"font-weight: bold\">         Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)          â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">58</span>)                  â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_72 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">15,104</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ leaky_re_lu_60 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_73 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ leaky_re_lu_61 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_74 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ leaky_re_lu_62 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_75 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ leaky_re_lu_63 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_76 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ leaky_re_lu_64 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_77 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">58</span>)                  â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">14,906</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer_12 (\u001b[38;5;33mInputLayer\u001b[0m)          â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m58\u001b[0m)                  â”‚               \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_72 (\u001b[38;5;33mDense\u001b[0m)                     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 â”‚          \u001b[38;5;34m15,104\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ leaky_re_lu_60 (\u001b[38;5;33mLeakyReLU\u001b[0m)           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 â”‚               \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_73 (\u001b[38;5;33mDense\u001b[0m)                     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 â”‚          \u001b[38;5;34m32,896\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ leaky_re_lu_61 (\u001b[38;5;33mLeakyReLU\u001b[0m)           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 â”‚               \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_74 (\u001b[38;5;33mDense\u001b[0m)                     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  â”‚           \u001b[38;5;34m8,256\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ leaky_re_lu_62 (\u001b[38;5;33mLeakyReLU\u001b[0m)           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  â”‚               \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_75 (\u001b[38;5;33mDense\u001b[0m)                     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 â”‚           \u001b[38;5;34m8,320\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ leaky_re_lu_63 (\u001b[38;5;33mLeakyReLU\u001b[0m)           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 â”‚               \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_76 (\u001b[38;5;33mDense\u001b[0m)                     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 â”‚          \u001b[38;5;34m33,024\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ leaky_re_lu_64 (\u001b[38;5;33mLeakyReLU\u001b[0m)           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 â”‚               \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_77 (\u001b[38;5;33mDense\u001b[0m)                     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m58\u001b[0m)                  â”‚          \u001b[38;5;34m14,906\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">112,506</span> (439.48 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m112,506\u001b[0m (439.48 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">112,506</span> (439.48 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m112,506\u001b[0m (439.48 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ğŸ‹ï¸â€â™‚ï¸ Training the Autoencoder ---\n",
      "Epoch 1/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 34ms/step - loss: 0.1276 - val_loss: 0.0479\n",
      "Epoch 2/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0348 - val_loss: 0.0250\n",
      "Epoch 3/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0177 - val_loss: 0.0192\n",
      "Epoch 4/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0135 - val_loss: 0.0148\n",
      "Epoch 5/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0111 - val_loss: 0.0129\n",
      "Epoch 6/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0092 - val_loss: 0.0110\n",
      "Epoch 7/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0080 - val_loss: 0.0098\n",
      "Epoch 8/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0071 - val_loss: 0.0095\n",
      "Epoch 9/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0069 - val_loss: 0.0089\n",
      "Epoch 10/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0063 - val_loss: 0.0087\n",
      "Epoch 11/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0060 - val_loss: 0.0089\n",
      "Epoch 12/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0061 - val_loss: 0.0087\n",
      "Epoch 13/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0062 - val_loss: 0.0085\n",
      "Epoch 14/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0057 - val_loss: 0.0083\n",
      "Epoch 15/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0061 - val_loss: 0.0083\n",
      "Epoch 16/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0063 - val_loss: 0.0082\n",
      "Epoch 17/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0054 - val_loss: 0.0075\n",
      "Epoch 18/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0052 - val_loss: 0.0075\n",
      "Epoch 19/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0056 - val_loss: 0.0077\n",
      "Epoch 20/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0052 - val_loss: 0.0072\n",
      "Epoch 21/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0047 - val_loss: 0.0069\n",
      "Epoch 22/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0039 - val_loss: 0.0065\n",
      "Epoch 23/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0035 - val_loss: 0.0065\n",
      "Epoch 24/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0040 - val_loss: 0.0064\n",
      "Epoch 25/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0038 - val_loss: 0.0063\n",
      "Epoch 26/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0036 - val_loss: 0.0064\n",
      "Epoch 27/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0035 - val_loss: 0.0061\n",
      "Epoch 28/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0036 - val_loss: 0.0060\n",
      "Epoch 29/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0032 - val_loss: 0.0060\n",
      "Epoch 30/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0035 - val_loss: 0.0062\n",
      "Epoch 31/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0032 - val_loss: 0.0061\n",
      "Epoch 32/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0034 - val_loss: 0.0059\n",
      "Epoch 33/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0033 - val_loss: 0.0059\n",
      "Epoch 34/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0033 - val_loss: 0.0058\n",
      "Epoch 35/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0033 - val_loss: 0.0059\n",
      "Epoch 36/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0033 - val_loss: 0.0059\n",
      "Epoch 37/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0031 - val_loss: 0.0058\n",
      "Epoch 38/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0031 - val_loss: 0.0058\n",
      "Epoch 39/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0031 - val_loss: 0.0059\n",
      "Epoch 40/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0030 - val_loss: 0.0056\n",
      "Epoch 41/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0032 - val_loss: 0.0059\n",
      "Epoch 42/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0033 - val_loss: 0.0053\n",
      "Epoch 43/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0029 - val_loss: 0.0049\n",
      "Epoch 44/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0029 - val_loss: 0.0047\n",
      "Epoch 45/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0031 - val_loss: 0.0048\n",
      "Epoch 46/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0031 - val_loss: 0.0053\n",
      "Epoch 47/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0029 - val_loss: 0.0053\n",
      "Epoch 48/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0029 - val_loss: 0.0047\n",
      "Epoch 49/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0024 - val_loss: 0.0042\n",
      "Epoch 50/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0022 - val_loss: 0.0042\n",
      "Epoch 51/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0022 - val_loss: 0.0038\n",
      "Epoch 52/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0021 - val_loss: 0.0040\n",
      "Epoch 53/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0023 - val_loss: 0.0038\n",
      "Epoch 54/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0019 - val_loss: 0.0038\n",
      "Epoch 55/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0023 - val_loss: 0.0039\n",
      "Epoch 56/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0020 - val_loss: 0.0038\n",
      "Epoch 57/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0020 - val_loss: 0.0038\n",
      "Epoch 58/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0018 - val_loss: 0.0037\n",
      "Epoch 59/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0019 - val_loss: 0.0038\n",
      "Epoch 60/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0019 - val_loss: 0.0037\n",
      "Epoch 61/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0019 - val_loss: 0.0036\n",
      "Epoch 62/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0020 - val_loss: 0.0035\n",
      "Epoch 63/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0019 - val_loss: 0.0036\n",
      "Epoch 64/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0017 - val_loss: 0.0036\n",
      "Epoch 65/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0018 - val_loss: 0.0033\n",
      "Epoch 66/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0017 - val_loss: 0.0030\n",
      "Epoch 67/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0018 - val_loss: 0.0030\n",
      "Epoch 68/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0014 - val_loss: 0.0032\n",
      "Epoch 69/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0016 - val_loss: 0.0031\n",
      "Epoch 70/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0017 - val_loss: 0.0063\n",
      "Epoch 71/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0020 - val_loss: 0.0045\n",
      "Epoch 72/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0027 - val_loss: 0.0040\n",
      "Epoch 73/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0023 - val_loss: 0.0048\n",
      "Epoch 74/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0021 - val_loss: 0.0033\n",
      "Epoch 75/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0019 - val_loss: 0.0031\n",
      "Epoch 76/200\n",
      "\u001b[1m53/53\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0015 - val_loss: 0.0033\n",
      "âœ… Model training complete.\n",
      "\n",
      "\n",
      "--- ğŸ¯ Finding Similar Properties ---\n",
      "\n",
      "--- ğŸ“ˆ Model Evaluation ---\n",
      "Final Mean Squared Error (Reconstruction Loss): 0.001757\n",
      "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\n",
      "Found 720 active properties to query.\n",
      "Found 1055 expired properties to match against.\n",
      "\n",
      "--- Computing KNN for Active Properties ---\n",
      "\n",
      "--- Creating Mapping for All Active Properties ---\n",
      "\n",
      "--- Example Match for Active Property Tag: recalling_0 ---\n",
      "Query Property Details:\n",
      "BHK                                                  2.0\n",
      "Property-Price                                      72.0\n",
      "City1                                          Ahmedabad\n",
      "Property-On-Floor                                    3.0\n",
      "Property-Facing                                     None\n",
      "Age-Of-Property                                      8.0\n",
      "Super-Built-up-Construction-Area                  1215.0\n",
      "Carpet-Construction-Area                           872.0\n",
      "Bathroom                                             2.0\n",
      "Furniture-Details                                   None\n",
      "Property-Status                                     None\n",
      "Current-Status                                      None\n",
      "Location                                    A-Chandkheda\n",
      "Parking-Details                                     None\n",
      "No-Of-Lift-Per-Block                                 2.0\n",
      "Service_Expiry_Date                                  NaT\n",
      "Tag                                          recalling_0\n",
      "Property_Type                                Residential\n",
      "Residential-Property                Flat-Apartment-Tower\n",
      "Commercial-Property-Type                            None\n",
      "Calculated_Status                                 Active\n",
      "Latitude                                         23.1091\n",
      "Longitude                                        72.5849\n",
      "Price_Per_SqFt                                  0.081175\n",
      "Name: 0, dtype: object\n",
      "\n",
      "Found 1 matches within 40.0% price range and 6.0 km.\n",
      "\n",
      "Rank 1: Expired Property Tag b-401-shubh-apartment-chandlodia-feb25_1040 (Index: 1040, Similarity: 0.7077, Distance: 4.90 km, Location: A-Chandlodia)\n",
      "BHK                                                                         2.0\n",
      "Property-Price                                                             50.5\n",
      "City1                                                                 Ahmedabad\n",
      "Property-On-Floor                                                           4.0\n",
      "Property-Facing                                                           South\n",
      "Age-Of-Property                                                            11.0\n",
      "Super-Built-up-Construction-Area                                         1260.0\n",
      "Carpet-Construction-Area                                                 1053.0\n",
      "Bathroom                                                                    1.0\n",
      "Furniture-Details                                                Semi-Furnished\n",
      "Property-Status                                                   Live-Property\n",
      "Current-Status                                                    Self-Occupied\n",
      "Location                                                           A-Chandlodia\n",
      "Parking-Details                                                  Common Parking\n",
      "No-Of-Lift-Per-Block                                                        1.0\n",
      "Service_Expiry_Date                                         2025-06-12 00:00:00\n",
      "Tag                                 b-401-shubh-apartment-chandlodia-feb25_1040\n",
      "Property_Type                                                       Residential\n",
      "Residential-Property                                       Flat-Apartment-Tower\n",
      "Commercial-Property-Type                                                   None\n",
      "Calculated_Status                                                       Expired\n",
      "Latitude                                                                 23.083\n",
      "Longitude                                                               72.5463\n",
      "Price_Per_SqFt                                                         0.047958\n",
      "Name: 1040, dtype: object\n",
      "\n",
      "âœ… Mapping saved to 'active_to_expired_mapping.json'\n",
      "Rejection reasons for unmatched properties saved to 'rejection_reasons.csv'\n",
      "\n",
      "--- ğŸ” Analyzing Unmatched Expired Properties ---\n",
      "Total Expired Properties: 1055\n",
      "Unmatched Expired Properties: 377\n",
      "Percentage of Expired Properties Unmatched: 35.73%\n",
      "Details of unmatched expired properties saved to 'unmatched_expired_properties.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hp\\AppData\\Local\\Temp\\ipykernel_14300\\508624454.py:544: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  unmatched_expired = df_cleaned[df_cleaned['Calculated_Status'] == 'Expired'][~df_cleaned['Tag'].isin([m['expired_tag'] for matches in active_to_expired_mapping.values() for m in matches])]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- ğŸ” Analyzing Unmatched Expired Properties ---\n",
      "Total Expired Properties: 1055\n",
      "Unmatched Expired Properties: 377\n",
      "Percentage of Expired Properties Unmatched: 35.73%\n",
      "Details of unmatched expired properties saved to 'unmatched_expired_properties.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "from geopy.geocoders import Nominatim\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import KMeans\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, LeakyReLU\n",
    "from keras.callbacks import EarlyStopping\n",
    "import math\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"\n",
    "    Calculate the great-circle distance between two points on Earth using the Haversine formula.\n",
    "    Parameters:\n",
    "        lat1, lon1: Latitude and longitude of the first point (in degrees).\n",
    "        lat2, lon2: Latitude and longitude of the second point (in degrees).\n",
    "    Returns:\n",
    "        Distance in kilometers.\n",
    "    \"\"\"\n",
    "    # Convert degrees to radians\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    \n",
    "    # Haversine formula\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat / 2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    R = 6371  # Earth's radius in km\n",
    "    return c * R\n",
    "\n",
    "def vincenty(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"\n",
    "    Calculate the geodesic distance between two points on the Earth using Vincenty's formula (in km).\n",
    "    More accurate than Haversine as it accounts for Earth's ellipsoidal shape.\n",
    "    \"\"\"\n",
    "    # WGS-84 ellipsoid parameters\n",
    "    a = 6378137.0  # semi-major axis in meters\n",
    "    f = 1 / 298.257223563  # flattening\n",
    "    b = a * (1 - f)  # semi-minor axis\n",
    "\n",
    "    # Convert degrees to radians\n",
    "    phi1 = math.radians(lat1)\n",
    "    lambda1 = math.radians(lon1)\n",
    "    phi2 = math.radians(lat2)\n",
    "    lambda2 = math.radians(lon2)\n",
    "\n",
    "    # Reduced latitudes\n",
    "    U1 = math.atan((1 - f) * math.tan(phi1))\n",
    "    U2 = math.atan((1 - f) * math.tan(phi2))\n",
    "\n",
    "    L = lambda2 - lambda1  # longitude difference\n",
    "    lam = L  # initial approximation of lambda\n",
    "\n",
    "    sinU1 = math.sin(U1)\n",
    "    cosU1 = math.cos(U1)\n",
    "    sinU2 = math.sin(U2)\n",
    "    cosU2 = math.cos(U2)\n",
    "\n",
    "    iter_limit = 100\n",
    "    for _ in range(iter_limit):\n",
    "        sin_lam = math.sin(lam)\n",
    "        cos_lam = math.cos(lam)\n",
    "\n",
    "        sin_sigma = math.sqrt((cosU2 * sin_lam)**2 + (cosU1 * sinU2 - sinU1 * cosU2 * cos_lam)**2)\n",
    "        if sin_sigma == 0:\n",
    "            return 0.0  # coincident points\n",
    "\n",
    "        cos_sigma = sinU1 * sinU2 + cosU1 * cosU2 * cos_lam\n",
    "        sigma = math.atan2(sin_sigma, cos_sigma)\n",
    "\n",
    "        sin_alpha = (cosU1 * cosU2 * sin_lam) / sin_sigma\n",
    "        cos_sq_alpha = 1 - sin_alpha**2\n",
    "\n",
    "        if cos_sq_alpha == 0:\n",
    "            return (a * math.pi) / 1000  # antipodal points, approximate\n",
    "\n",
    "        cos_2sigma_m = cos_sigma - (2 * sinU1 * sinU2) / cos_sq_alpha\n",
    "\n",
    "        C = f / 16 * cos_sq_alpha * (4 + f * (4 - 3 * cos_sq_alpha))\n",
    "\n",
    "        lam_prev = lam\n",
    "        lam = L + (1 - C) * f * sin_alpha * (sigma + C * sin_sigma * (cos_2sigma_m + C * cos_sigma * (-1 + 2 * cos_2sigma_m**2)))\n",
    "\n",
    "        if abs(lam - lam_prev) < 1e-12:\n",
    "            break\n",
    "    else:\n",
    "        raise ValueError(\"Vincenty formula failed to converge after 100 iterations\")\n",
    "\n",
    "    u_sq = cos_sq_alpha * (a**2 - b**2) / (b**2)\n",
    "    A = 1 + u_sq / 16384 * (4096 + u_sq * (-768 + u_sq * (320 - 175 * u_sq)))\n",
    "    B = u_sq / 1024 * (256 + u_sq * (-128 + u_sq * (74 - 47 * u_sq)))\n",
    "\n",
    "    delta_sigma = B * sin_sigma * (cos_2sigma_m + B / 4 * (cos_sigma * (-1 + 2 * cos_2sigma_m**2) - B / 6 * cos_2sigma_m * (-3 + 4 * sin_sigma**2) * (-3 + 4 * cos_2sigma_m**2)))\n",
    "\n",
    "    s = b * A * (sigma - delta_sigma) / 1000  # distance in km\n",
    "\n",
    "    return s\n",
    "\n",
    "def get_or_create_geo_mapping(df):\n",
    "    \"\"\"\n",
    "    Loads geocoding from a file or creates it by calling an API.\n",
    "    \"\"\"\n",
    "    json_path = 'geo_mapping.json'\n",
    "    if os.path.exists(json_path):\n",
    "        print(f\"âœ… Found existing geocoding file at '{json_path}'. Loading...\")\n",
    "        with open(json_path, 'r') as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    print(f\"âš ï¸ No geocoding file found. Creating a new one using geopy...\")\n",
    "    print(\"This will take several minutes as it respects API rate limits.\")\n",
    "\n",
    "    geolocator = Nominatim(user_agent=\"property_matcher_app_v6\")\n",
    "    unique_locations = df['Location'].dropna().unique()\n",
    "    geo_mapping = {}\n",
    "\n",
    "    city_prefix_map = {\n",
    "        'A': 'Ahmedabad', 'P': 'Pune', 'G': 'Gandhinagar',\n",
    "        'S': 'Surat', 'V': 'Vadodara', 'B': 'Vadodara'\n",
    "    }\n",
    "\n",
    "    for location_str in unique_locations:\n",
    "        if not isinstance(location_str, str) or '-' not in location_str:\n",
    "            continue\n",
    "\n",
    "        parts = location_str.split('-', 1)\n",
    "        prefix = parts[0].strip()\n",
    "        area = parts[1].strip()\n",
    "        city = city_prefix_map.get(prefix, 'Ahmedabad')\n",
    "\n",
    "        query = f\"{area}, {city}, India\"\n",
    "        try:\n",
    "            location_data = geolocator.geocode(query, timeout=10)\n",
    "            if location_data:\n",
    "                geo_mapping[location_str] = (location_data.latitude, location_data.longitude)\n",
    "                print(f\"âœ… Found: {query} -> ({location_data.latitude:.4f}, {location_data.longitude:.4f})\")\n",
    "            else:\n",
    "                print(f\"âš ï¸ Not Found: {query}\")\n",
    "            time.sleep(1)\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error for '{query}': {e}\")\n",
    "\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(geo_mapping, f, indent=4)\n",
    "    print(f\"âœ… Geocoding complete. Saved to '{json_path}'.\")\n",
    "    return geo_mapping\n",
    "\n",
    "def run_data_pipeline():\n",
    "    \"\"\"\n",
    "    Loads, cleans, and preprocesses the property data, using manually created geo_mapping.json.\n",
    "    Keeps original Location prefixes, trims whitespace, and ensures all locations are in geo_mapping.json.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        property_df = pd.read_csv('PropertyData.csv', low_memory=False)\n",
    "        print(\"âœ… Successfully loaded PropertyData.csv\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"âŒ Error: 'PropertyData.csv' not found.\")\n",
    "        return None, None, None, None\n",
    "\n",
    "    df = property_df.copy()\n",
    "    features_to_use = [\n",
    "        'BHK', 'Property-Price', 'City1', 'Property-On-Floor', 'Property-Facing',\n",
    "        'Age-Of-Property', 'Super-Built-up-Construction-Area', 'Carpet-Construction-Area',\n",
    "        'Bathroom', 'Furniture-Details', 'Property-Status', 'Current-Status', 'Location',\n",
    "        'Parking-Details', 'No-Of-Lift-Per-Block', 'Service_Expiry_Date', 'Tag',\n",
    "        'Property_Type', 'Residential-Property', 'Commercial-Property-Type'\n",
    "    ]\n",
    "    missing_cols = [col for col in features_to_use if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        print(f\"âš ï¸ Warning: Columns {missing_cols} not found in PropertyData.csv. Proceeding without them.\")\n",
    "        features_to_use = [col for col in features_to_use if col in df.columns]\n",
    "    df = df[features_to_use]\n",
    "    print(f\"âœ… Selected {len(features_to_use)} features for modeling.\")\n",
    "\n",
    "    # Clean Location values\n",
    "    print(\"\\nğŸ§¹ Cleaning Location values...\")\n",
    "    df['Location'] = df['Location'].astype(str).str.strip()  # Trim whitespace\n",
    "    invalid_locations = df[df['Location'] == '-']\n",
    "    if not invalid_locations.empty:\n",
    "        print(f\"âš ï¸ Found {len(invalid_locations)} rows with Location = '-'. Setting to NaN.\")\n",
    "        df.loc[df['Location'] == '-', 'Location'] = np.nan\n",
    "\n",
    "    # Clean Property_Type-related columns\n",
    "    if 'Property_Type' in df.columns:\n",
    "        print(\"\\nğŸ§¹ Cleaning Property_Type, Residential-Property, Commercial-Property-Type...\")\n",
    "        if 'Commercial-Property-Type' in df.columns:\n",
    "            df.loc[df['Property_Type'] == 'Residential', 'Commercial-Property-Type'] = np.nan\n",
    "        if 'Residential-Property' in df.columns:\n",
    "            df.loc[df['Property_Type'] == 'Commercial', 'Residential-Property'] = np.nan\n",
    "        print(\"Inconsistent Residential properties with Commercial-Property-Type:\")\n",
    "        print(df[(df['Property_Type'] == 'Residential') & (df['Commercial-Property-Type'].notna())][['Tag', 'Property_Type', 'Residential-Property', 'Commercial-Property-Type']])\n",
    "        print(\"Inconsistent Commercial properties with Residential-Property:\")\n",
    "        print(df[(df['Property_Type'] == 'Commercial') & (df['Residential-Property'].notna())][['Tag', 'Property_Type', 'Residential-Property', 'Commercial-Property-Type']])\n",
    "\n",
    "    # Status Calculation\n",
    "    print(\"\\nğŸ—“ï¸ Calculating property status (Active/Expired/Sold)...\")\n",
    "    sold_statuses = ['Sold-CD', 'Sold-Others', 'Rented-CD']\n",
    "    current_date = pd.to_datetime('2025-08-23')\n",
    "    df['Service_Expiry_Date'] = pd.to_datetime(df['Service_Expiry_Date'], errors='coerce', dayfirst=True)\n",
    "    \n",
    "    print(\"Null Service_Expiry_Date count:\", df['Service_Expiry_Date'].isna().sum())\n",
    "    print(\"Sample Service_Expiry_Date values:\")\n",
    "    print(df[['Tag', 'Service_Expiry_Date']].head())\n",
    "\n",
    "    conditions = [\n",
    "        df['Property-Status'].isin(sold_statuses),\n",
    "        df['Service_Expiry_Date'] < current_date\n",
    "    ]\n",
    "    choices = ['Sold', 'Expired']\n",
    "    df['Calculated_Status'] = np.select(conditions, choices, default='Active')\n",
    "    print(f\"âœ… Status calculation complete. Status distribution:\")\n",
    "    print(df['Calculated_Status'].value_counts())\n",
    "\n",
    "    print(\"\\nChecking for duplicate Tags...\")\n",
    "    if df['Tag'].duplicated().any():\n",
    "        print(f\"Found {df['Tag'].duplicated().sum()} duplicate Tags. Assigning unique Tags...\")\n",
    "        duplicate_tags = df[df['Tag'].duplicated(keep=False)]\n",
    "        duplicate_tags[['Tag', 'Calculated_Status', 'Property-Price', 'Location']].to_csv('duplicate_tags.csv')\n",
    "        print(\"Exported duplicate Tags to 'duplicate_tags.csv' for review.\")\n",
    "        df['Tag'] = df['Tag'].astype(str) + '_' + df.index.astype(str)\n",
    "        print(\"Assigned unique Tags by appending index.\")\n",
    "\n",
    "    # Load manually created geo_mapping.json\n",
    "    print(\"\\nğŸ“ Loading geo_mapping.json...\")\n",
    "    try:\n",
    "        with open('geo_mapping.json', 'r') as f:\n",
    "            geo_mapping = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(\"âŒ Error: 'geo_mapping.json' not found.\")\n",
    "        return None, None, None, None\n",
    "\n",
    "    # Validate locations\n",
    "    valid_locations = df['Location'].astype(str).str.strip()\n",
    "    missing_locations = set(valid_locations) - set(geo_mapping.keys()) - {'nan'}\n",
    "    if missing_locations:\n",
    "        print(f\"âŒ Error: {len(missing_locations)} locations not in geo_mapping.json: {missing_locations}\")\n",
    "        print(\"Please update geo_mapping.json with these locations before proceeding.\")\n",
    "        return None, None, None, None\n",
    "\n",
    "    df[['Latitude', 'Longitude']] = df['Location'].apply(\n",
    "        lambda x: pd.Series(geo_mapping.get(str(x).strip(), (np.nan, np.nan)) if str(x).strip() != 'nan' else (np.nan, np.nan))\n",
    "    )\n",
    "    print(\"Coordinate assignment complete. Sample coordinates:\")\n",
    "    print(df[['Location', 'Latitude', 'Longitude']].head())\n",
    "\n",
    "    print(\"\\nâš™ï¸ Starting data cleaning process...\")\n",
    "    def clean_price(price):\n",
    "        if not isinstance(price, str): return np.nan\n",
    "        price_str = price.lower()\n",
    "        try:\n",
    "            numbers = re.findall(r'[\\d\\.]+', price_str)\n",
    "            if not numbers: return np.nan\n",
    "            value = float(numbers[0])\n",
    "            if 'cr' in price_str: return value * 100\n",
    "            return value\n",
    "        except (ValueError, IndexError): return np.nan\n",
    "\n",
    "    def clean_area(area):\n",
    "        if not isinstance(area, str): return np.nan\n",
    "        area_str = area.lower()\n",
    "        try:\n",
    "            numbers = re.findall(r'[\\d\\.]+', area_str)\n",
    "            if not numbers: return np.nan\n",
    "            value = float(numbers[0])\n",
    "            if 'yard' in area_str: return value * 9\n",
    "            return value\n",
    "        except (ValueError, IndexError): return np.nan\n",
    "\n",
    "    def clean_floor(floor):\n",
    "        if not isinstance(floor, str): return np.nan\n",
    "        floor_str = floor.lower().replace('g', '0')\n",
    "        try:\n",
    "            numbers = re.findall(r'\\d+', floor_str)\n",
    "            if numbers: return int(numbers[0])\n",
    "            return np.nan\n",
    "        except (ValueError, IndexError): return np.nan\n",
    "\n",
    "    def clean_age(age):\n",
    "        if not isinstance(age, str): return np.nan\n",
    "        age_str = age.lower()\n",
    "        if 'new' in age_str or 'under' in age_str: return 0\n",
    "        try:\n",
    "            numbers = [int(s) for s in re.findall(r'\\d+', age_str)]\n",
    "            if numbers: return sum(numbers) / len(numbers)\n",
    "            return np.nan\n",
    "        except (ValueError, IndexError): return np.nan\n",
    "\n",
    "    df.replace('-', np.nan, inplace=True)\n",
    "    df.replace('NA', np.nan, inplace=True)\n",
    "    df['Property-Price'] = df['Property-Price'].apply(clean_price)\n",
    "    df['Super-Built-up-Construction-Area'] = df['Super-Built-up-Construction-Area'].apply(clean_area)\n",
    "    df['Carpet-Construction-Area'] = df['Carpet-Construction-Area'].apply(clean_area)\n",
    "    df['Property-On-Floor'] = df['Property-On-Floor'].apply(clean_floor)\n",
    "    df['Age-Of-Property'] = df['Age-Of-Property'].apply(clean_age)\n",
    "    df['BHK'] = pd.to_numeric(df['BHK'].astype(str).str.extract(r'(\\d+)', expand=False), errors='coerce')\n",
    "    df['Bathroom'] = pd.to_numeric(df['Bathroom'], errors='coerce')\n",
    "    df['No-Of-Lift-Per-Block'] = pd.to_numeric(df['No-Of-Lift-Per-Block'], errors='coerce')\n",
    "    df.loc[df['Bathroom'] > 20, 'Bathroom'] = np.nan\n",
    "\n",
    "    df['Price_Per_SqFt'] = df['Property-Price'] / df['Carpet-Construction-Area'].clip(lower=1)\n",
    "\n",
    "    numerical_cols = ['Property-Price', 'Super-Built-up-Construction-Area', 'Carpet-Construction-Area',\n",
    "                      'Property-On-Floor', 'Age-Of-Property', 'BHK', 'Bathroom', 'No-Of-Lift-Per-Block',\n",
    "                      'Latitude', 'Longitude', 'Price_Per_SqFt']\n",
    "    for col in numerical_cols:\n",
    "        df[col].fillna(df[col].median(), inplace=True)\n",
    "\n",
    "    categorical_cols = ['City1', 'Property-Facing', 'Furniture-Details', 'Property-Status',\n",
    "                       'Current-Status', 'Parking-Details', 'Calculated_Status',\n",
    "                       'Property_Type', 'Residential-Property', 'Commercial-Property-Type']\n",
    "    for col in categorical_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str)\n",
    "            df[col].replace('nan', 'None', inplace=True)\n",
    "            df[col].fillna('None', inplace=True)\n",
    "\n",
    "    print(\"âœ… Data cleaning and imputation complete.\")\n",
    "    print(\"Property_Type distribution:\")\n",
    "    print(df['Property_Type'].value_counts())\n",
    "    print(\"Residential-Property distribution:\")\n",
    "    print(df['Residential-Property'].value_counts())\n",
    "    print(\"Commercial-Property-Type distribution:\")\n",
    "    print(df['Commercial-Property-Type'].value_counts())\n",
    "\n",
    "    numerical_features = [col for col in numerical_cols if col in df.columns]\n",
    "    categorical_features = [col for col in categorical_cols if col != 'Calculated_Status' and col in df.columns]\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', MinMaxScaler(), numerical_features),\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "        ],\n",
    "        remainder='drop'\n",
    "    )\n",
    "    \n",
    "    return df, numerical_cols, categorical_cols, preprocessor\n",
    "\n",
    "\n",
    "    \n",
    "def perform_eda(df_cleaned):\n",
    "    print(\"\\n\\n--- ğŸ”¬ Starting Exploratory Data Analysis ---\")\n",
    "    print(\"\\n--- ğŸ“Š Descriptive Statistics ---\")\n",
    "    print(df_cleaned.describe())\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    print(\"\\n--- ğŸ“ˆ Visualizing Numerical Distributions ---\")\n",
    "    df_cleaned.hist(bins=30, figsize=(20, 15), layout=(4, 3))\n",
    "    plt.suptitle('Distribution of Numerical Features', size=20, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Block 3- Training the model\n",
    "def train_autoencoder(df_cleaned, preprocessor):\n",
    "    \"\"\"\n",
    "    Builds and trains the tuned autoencoder model using the provided preprocessor.\n",
    "    Added early stopping for better training efficiency.\n",
    "    \"\"\"\n",
    "    print(\"\\n\\n--- ğŸ¤– Preparing data for the model ---\")\n",
    "    \n",
    "    # Use the preprocessor that was created in the data pipeline\n",
    "    X_processed = preprocessor.fit_transform(df_cleaned)\n",
    "    print(f\"âœ… Preprocessing complete. Shape of model input data: {X_processed.shape}\")\n",
    "\n",
    "    input_dim = X_processed.shape[1]\n",
    "    embedding_dim = 64\n",
    "\n",
    "    # --- Model Architecture ---\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    encoder = Dense(256)(input_layer)\n",
    "    encoder = LeakyReLU()(encoder)\n",
    "    encoder = Dense(128)(encoder)\n",
    "    encoder = LeakyReLU()(encoder)\n",
    "    \n",
    "    bottleneck = Dense(embedding_dim)(encoder)\n",
    "    bottleneck = LeakyReLU()(bottleneck)\n",
    "\n",
    "    decoder = Dense(128)(bottleneck)\n",
    "    decoder = LeakyReLU()(decoder)\n",
    "    decoder = Dense(256)(decoder)\n",
    "    decoder = LeakyReLU()(decoder)\n",
    "    output_layer = Dense(input_dim, activation='sigmoid')(decoder)\n",
    "\n",
    "    autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "    encoder_model = Model(inputs=input_layer, outputs=bottleneck)\n",
    "    autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    print(\"\\nâœ… Tuned autoencoder model built.\")\n",
    "    autoencoder.summary()\n",
    "\n",
    "    print(\"\\n--- ğŸ‹ï¸â€â™‚ï¸ Training the Autoencoder ---\")\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    autoencoder.fit(\n",
    "        X_processed, X_processed,\n",
    "        epochs=200,  # Increased max epochs, but early stopping will prevent overfitting\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "        validation_split=0.1,\n",
    "        verbose=1,\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    print(\"âœ… Model training complete.\")\n",
    "    return encoder_model, autoencoder, X_processed\n",
    "\n",
    "\n",
    "def find_similar_properties(encoder_model, autoencoder, X_processed, df_cleaned):\n",
    "    \"\"\"\n",
    "    Creates a mapping from each active property to similar expired properties.\n",
    "    Uses Haversine formula for distance and strict geographic filter.\n",
    "    \"\"\"\n",
    "    print(\"\\n\\n--- ğŸ¯ Finding Similar Properties ---\")\n",
    "    \n",
    "    if df_cleaned['Tag'].duplicated().any():\n",
    "        raise ValueError(\"Duplicate values found in Tag column; it must be unique.\")\n",
    "    if df_cleaned['Tag'].isna().any():\n",
    "        raise ValueError(\"Null values found in Tag column; all properties must have a Tag.\")\n",
    "    \n",
    "    assert len(X_processed) == len(df_cleaned), \"Mismatch between X_processed and df_cleaned rows\"\n",
    "    \n",
    "    loss = autoencoder.evaluate(X_processed, X_processed, verbose=0)\n",
    "    print(f\"\\n--- ğŸ“ˆ Model Evaluation ---\")\n",
    "    print(f\"Final Mean Squared Error (Reconstruction Loss): {loss:.6f}\")\n",
    "    \n",
    "    all_embeddings = encoder_model.predict(X_processed)\n",
    "    assert len(all_embeddings) == len(df_cleaned), \"Embedding row count mismatch\"\n",
    "\n",
    "    query_mask = df_cleaned['Calculated_Status'] == 'Active'\n",
    "    database_mask = df_cleaned['Calculated_Status'] == 'Expired'\n",
    "\n",
    "    query_indices = df_cleaned[query_mask].index\n",
    "    database_indices = df_cleaned[database_mask].index\n",
    "    \n",
    "    query_positions = df_cleaned.index.get_indexer(query_indices)\n",
    "    database_positions = df_cleaned.index.get_indexer(database_indices)\n",
    "    \n",
    "    active_embeddings = all_embeddings[query_positions]\n",
    "    database_embeddings = all_embeddings[database_positions]\n",
    "\n",
    "    print(f\"\\nFound {len(query_indices)} active properties to query.\")\n",
    "    print(f\"Found {len(database_indices)} expired properties to match against.\")\n",
    "\n",
    "    active_to_expired_mapping = {}\n",
    "    rejection_reasons = {}\n",
    "\n",
    "    if len(database_indices) > 0 and len(query_indices) > 0:\n",
    "        K = 20\n",
    "        min_similarity_threshold = 0.6\n",
    "        max_distance_km = 6.0\n",
    "        price_tolerance = 0.40\n",
    "        max_matches = 10\n",
    "        \n",
    "        knn = NearestNeighbors(n_neighbors=min(K, len(database_embeddings)), metric='cosine')\n",
    "        knn.fit(database_embeddings)\n",
    "\n",
    "        print(\"\\n--- Computing KNN for Active Properties ---\")\n",
    "        distances, indices = knn.kneighbors(active_embeddings)\n",
    "\n",
    "        print(\"\\n--- Creating Mapping for All Active Properties ---\")\n",
    "        for i, active_idx in enumerate(query_indices):\n",
    "            active_tag = str(df_cleaned.loc[active_idx, 'Tag'])\n",
    "            active_price = df_cleaned.loc[active_idx, 'Property-Price']\n",
    "            active_lat = df_cleaned.loc[active_idx, 'Latitude']\n",
    "            active_lon = df_cleaned.loc[active_idx, 'Longitude']\n",
    "            active_location = df_cleaned.loc[active_idx, 'Location']\n",
    "            active_property_type = df_cleaned.loc[active_idx, 'Property_Type'] if 'Property_Type' in df_cleaned.columns else None\n",
    "            \n",
    "            lower_bound = active_price * (1 - price_tolerance)\n",
    "            upper_bound = active_price * (1 + price_tolerance)\n",
    "            \n",
    "            filtered_matches = []\n",
    "            for j, candidate_idx in enumerate(indices[i]):\n",
    "                expired_position = candidate_idx\n",
    "                expired_idx = database_indices[expired_position]\n",
    "                expired_tag = str(df_cleaned.loc[expired_idx, 'Tag'])\n",
    "                expired_price = df_cleaned.loc[expired_idx, 'Property-Price']\n",
    "                expired_lat = df_cleaned.loc[expired_idx, 'Latitude']\n",
    "                expired_lon = df_cleaned.loc[expired_idx, 'Longitude']\n",
    "                expired_location = df_cleaned.loc[expired_idx, 'Location']\n",
    "                expired_property_type = df_cleaned.loc[expired_idx, 'Property_Type'] if 'Property_Type' in df_cleaned.columns else None\n",
    "                \n",
    "                dist_km = haversine(active_lat, active_lon, expired_lat, expired_lon)\n",
    "                similarity = 1 - distances[i][j]\n",
    "                \n",
    "                reasons = []\n",
    "                if similarity < min_similarity_threshold:\n",
    "                    reasons.append(f\"Similarity {similarity:.4f} < {min_similarity_threshold}\")\n",
    "                if not (lower_bound <= expired_price <= upper_bound):\n",
    "                    reasons.append(f\"Price {expired_price} outside [{lower_bound:.2f}, {upper_bound:.2f}]\")\n",
    "                if dist_km > max_distance_km:\n",
    "                    reasons.append(f\"Distance {dist_km:.2f} km ({active_location} to {expired_location}) > {max_distance_km} km\")\n",
    "                if active_property_type and expired_property_type and active_property_type != expired_property_type:\n",
    "                    reasons.append(f\"Property_Type mismatch: {active_property_type} != {expired_property_type}\")\n",
    "                \n",
    "                if reasons:\n",
    "                    rejection_reasons.setdefault(expired_tag, []).extend(reasons)\n",
    "                \n",
    "                if (lower_bound <= expired_price <= upper_bound and \n",
    "                    dist_km <= max_distance_km and \n",
    "                    similarity >= min_similarity_threshold and\n",
    "                    (not active_property_type or active_property_type == expired_property_type)):\n",
    "                    filtered_matches.append({\n",
    "                        'expired_tag': expired_tag,\n",
    "                        'expired_index': int(expired_idx),\n",
    "                        'similarity': float(similarity),\n",
    "                        'distance_km': float(dist_km),\n",
    "                        'expired_location': expired_location\n",
    "                    })\n",
    "            \n",
    "            filtered_matches.sort(key=lambda x: x['similarity'], reverse=True)\n",
    "            active_to_expired_mapping[active_tag] = filtered_matches[:max_matches]\n",
    "            \n",
    "            if i == 0:\n",
    "                print(f\"\\n--- Example Match for Active Property Tag: {active_tag} ---\")\n",
    "                print(\"Query Property Details:\")\n",
    "                print(df_cleaned.loc[active_idx])\n",
    "                print(f\"\\nFound {len(filtered_matches)} matches within {price_tolerance*100}% price range and {max_distance_km} km.\")\n",
    "                for rank, match in enumerate(filtered_matches[:max_matches], 1):\n",
    "                    print(f\"\\nRank {rank}: Expired Property Tag {match['expired_tag']} (Index: {match['expired_index']}, Similarity: {match['similarity']:.4f}, Distance: {match['distance_km']:.2f} km, Location: {match['expired_location']})\")\n",
    "                    print(df_cleaned.loc[match['expired_index']])\n",
    "    \n",
    "    else:\n",
    "        print(\"\\nCould not perform matching: no active or expired properties found.\")\n",
    "    \n",
    "    with open('active_to_expired_mapping.json', 'w') as f:\n",
    "        json.dump(active_to_expired_mapping, f, indent=4)\n",
    "    print(\"\\nâœ… Mapping saved to 'active_to_expired_mapping.json'\")\n",
    "\n",
    "    rejection_data = []\n",
    "    for expired_tag, reasons in rejection_reasons.items():\n",
    "        rejection_data.append({\n",
    "            'expired_tag': expired_tag,\n",
    "            'rejection_reasons': '; '.join(set(reasons))\n",
    "        })\n",
    "    rejection_df = pd.DataFrame(rejection_data)\n",
    "    rejection_df.to_csv('rejection_reasons.csv', index=False)\n",
    "    print(\"Rejection reasons for unmatched properties saved to 'rejection_reasons.csv'\")\n",
    "\n",
    "    unmatched_expired = df_cleaned[df_cleaned['Calculated_Status'] == 'Expired'][~df_cleaned['Tag'].isin([m['expired_tag'] for matches in active_to_expired_mapping.values() for m in matches])]\n",
    "    columns_to_save = ['Tag', 'Property_Type', 'Location', 'BHK', 'Property-Price', 'Carpet-Construction-Area', 'Service_Expiry_Date', 'Residential-Property', 'Commercial-Property-Type', 'Calculated_Status', 'Latitude', 'Longitude']\n",
    "    columns_to_save = [col for col in columns_to_save if col in df_cleaned.columns]\n",
    "    unmatched_expired[columns_to_save].to_csv('unmatched_expired_properties.csv', index=False)\n",
    "    \n",
    "    print(f\"\\n--- ğŸ” Analyzing Unmatched Expired Properties ---\")\n",
    "    print(f\"Total Expired Properties: {len(df_cleaned[df_cleaned['Calculated_Status'] == 'Expired'])}\")\n",
    "    print(f\"Unmatched Expired Properties: {len(unmatched_expired)}\")\n",
    "    print(f\"Percentage of Expired Properties Unmatched: {len(unmatched_expired) / len(df_cleaned[df_cleaned['Calculated_Status'] == 'Expired']) * 100:.2f}%\")\n",
    "    print(\"Details of unmatched expired properties saved to 'unmatched_expired_properties.csv'\")\n",
    "\n",
    "    return all_embeddings, active_to_expired_mapping\n",
    "\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df_cleaned, numerical_cols, categorical_cols, preprocessor = run_data_pipeline()\n",
    "    if df_cleaned is not None:\n",
    "        # perform_eda(df_cleaned)  # Uncomment if you want to run EDA\n",
    "        encoder_model, autoencoder, X_processed = train_autoencoder(df_cleaned, preprocessor)\n",
    "        all_embeddings, mapping = find_similar_properties(encoder_model, autoencoder, X_processed, df_cleaned)\n",
    "\n",
    "        # --- Analysis of Unmatched Expired Properties ---\n",
    "        print(\"\\n\\n--- ğŸ” Analyzing Unmatched Expired Properties ---\")\n",
    "        \n",
    "        # Get all expired properties' Tags\n",
    "        expired_mask = df_cleaned['Calculated_Status'] == 'Expired'\n",
    "        expired_tags = set(df_cleaned[expired_mask]['Tag'].astype(str))\n",
    "        total_expired = len(expired_tags)\n",
    "        print(f\"Total Expired Properties: {total_expired}\")\n",
    "\n",
    "        # Get all matched expired Tags from the mapping\n",
    "        matched_expired_tags = set()\n",
    "        for active_tag, matches in mapping.items():\n",
    "            for match in matches:\n",
    "                matched_expired_tags.add(match['expired_tag'])\n",
    "\n",
    "        # Find unmatched expired properties\n",
    "        unmatched_expired_tags = expired_tags - matched_expired_tags\n",
    "        num_unmatched = len(unmatched_expired_tags)\n",
    "        unmatched_percentage = (num_unmatched / total_expired * 100) if total_expired > 0 else 0\n",
    "\n",
    "        print(f\"Unmatched Expired Properties: {num_unmatched}\")\n",
    "        print(f\"Percentage of Expired Properties Unmatched: {unmatched_percentage:.2f}%\")\n",
    "\n",
    "        # Save unmatched expired properties to CSV for review\n",
    "        if num_unmatched > 0:\n",
    "            unmatched_df = df_cleaned[df_cleaned['Tag'].isin(unmatched_expired_tags)][\n",
    "                ['Tag', 'Property-Price', 'Location', 'BHK', 'Carpet-Construction-Area', 'Service_Expiry_Date']\n",
    "            ]\n",
    "            unmatched_df.to_csv('unmatched_expired_properties.csv', index=False)\n",
    "            print(\"Details of unmatched expired properties saved to 'unmatched_expired_properties.csv'\")\n",
    "        else:\n",
    "            print(\"All expired properties were matched to at least one active property.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "484ba618-7fab-42ef-88f8-a6b780a43ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Unmatched Properties: 366\n",
      "\n",
      "Available Columns: ['Tag', 'Property-Price', 'Location', 'BHK', 'Carpet-Construction-Area', 'Service_Expiry_Date']\n",
      "\n",
      "Error: Property_Type column not found. Available columns: ['Tag', 'Property-Price', 'Location', 'BHK', 'Carpet-Construction-Area', 'Service_Expiry_Date']\n",
      "\n",
      "Location distribution (top 10):\n",
      "Location\n",
      "A-Gota           18\n",
      "A-Naroda         16\n",
      "A-Chandkheda     15\n",
      "A-Vaishnodevi    11\n",
      "G-Kudasan        10\n",
      "A-Vatva          10\n",
      "A-Bopal           9\n",
      "A-Adalaj          9\n",
      "A-Bapunagar       8\n",
      "A-Kasindra        8\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Pune-based Unmatched Properties: 0\n",
      "Series([], Name: count, dtype: int64)\n",
      "\n",
      "BHK distribution (percentage):\n",
      "BHK\n",
      "3.0    58.196721\n",
      "2.0    21.584699\n",
      "4.0     8.196721\n",
      "1.0     7.923497\n",
      "5.0     4.098361\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Property-Price summary:\n",
      "count    3.660000e+02\n",
      "mean     2.472618e+04\n",
      "std      4.704306e+05\n",
      "min      1.000000e+00\n",
      "25%      3.725000e+01\n",
      "50%      7.000000e+01\n",
      "75%      1.350000e+02\n",
      "max      9.000000e+06\n",
      "Name: Property-Price, dtype: float64\n",
      "\n",
      "Carpet-Construction-Area summary:\n",
      "count      366.000000\n",
      "mean      1293.245902\n",
      "std       4259.608967\n",
      "min         70.000000\n",
      "25%        740.250000\n",
      "50%        872.000000\n",
      "75%       1134.000000\n",
      "max      81000.000000\n",
      "Name: Carpet-Construction-Area, dtype: float64\n",
      "\n",
      "Service_Expiry_Date summary:\n",
      "count                              366\n",
      "mean     2025-04-24 00:27:32.459016192\n",
      "min                2024-10-25 00:00:00\n",
      "25%                2025-03-07 06:00:00\n",
      "50%                2025-05-05 00:00:00\n",
      "75%                2025-06-16 18:00:00\n",
      "max                2025-08-19 00:00:00\n",
      "Name: Service_Expiry_Date, dtype: object\n",
      "\n",
      "Commercial-Property-Type or Property_Type column not found.\n",
      "\n",
      "Visualizations saved to 'top_locations_unmatched.png', 'price_by_type_unmatched.png', 'bhk_distribution_unmatched.png', and 'price_vs_area_unmatched.png'\n",
      "\n",
      "Common Patterns Summary:\n",
      "- Skew in Property_Type: If >50% are Commercial, add Commercial-specific active properties or weight Commercial-Property-Type in the autoencoder.\n",
      "- Skew in Location: If Pune-based (P-) locations dominate (>30%), add Pune-specific active properties or relax max_distance_km to 10.0.\n",
      "- Price Range: If unmatched properties are in high/low price extremes (e.g., >100 or <10), increase price_tolerance to 0.60.\n",
      "- BHK or Area: If skewed (e.g., many 4+ BHK or >2000 sq.ft.), add interaction features (e.g., BHK * Carpet-Construction-Area) to numerical_cols.\n",
      "- Recent Expirations: If many recent Service_Expiry_Date (e.g., post-2024), prioritize them in matching by adding a recency weight to similarity.\n",
      "\n",
      "Modeling Recommendations:\n",
      "- Add missing locations (V-Nizampuera, V-Atladara, V-Aklkapuri) to geo_mapping.json with verified coordinates.\n",
      "- If Commercial skew: Use TargetEncoder for Commercial-Property-Type in run_data_pipeline to capture value-based encoding.\n",
      "- If Pune skew: Derive a 'City' feature (e.g., 'Pune' for P- locations) and add to categorical_cols.\n",
      "- To reduce unmatched (371 â†’ ~80): Set max_distance_km=10.0, price_tolerance=0.60, min_similarity_threshold=0.45, K=100 in find_similar_properties.\n",
      "- Retrain autoencoder with Dropout(0.2) after Dense layers and EarlyStopping(patience=20) if similarity rejections are high in rejection_reasons.csv.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load unmatched_expired_properties.csv\n",
    "try:\n",
    "    df = pd.read_csv('unmatched_expired_properties.csv')\n",
    "    print(\"Total Unmatched Properties:\", len(df))\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'unmatched_expired_properties.csv' not found.\")\n",
    "    exit()\n",
    "\n",
    "# Check available columns\n",
    "print(\"\\nAvailable Columns:\", list(df.columns))\n",
    "\n",
    "# Property_Type distribution (handle potential name variations)\n",
    "property_type_col = None\n",
    "for col in ['Property_Type', 'Property-Type', 'property_type']:\n",
    "    if col in df.columns:\n",
    "        property_type_col = col\n",
    "        break\n",
    "\n",
    "if property_type_col:\n",
    "    print(f\"\\n{property_type_col} distribution (percentage):\")\n",
    "    print(df[property_type_col].value_counts(normalize=True) * 100)\n",
    "else:\n",
    "    print(\"\\nError: Property_Type column not found. Available columns:\", list(df.columns))\n",
    "\n",
    "# Location distribution (top 10)\n",
    "print(\"\\nLocation distribution (top 10):\")\n",
    "print(df['Location'].value_counts().head(10))\n",
    "\n",
    "# Check for Pune-based locations (assuming Pune locations start with 'P-')\n",
    "pune_locations = df[df['Location'].str.startswith('P-', na=False)]\n",
    "print(\"\\nPune-based Unmatched Properties:\", len(pune_locations))\n",
    "print(pune_locations['Location'].value_counts())\n",
    "\n",
    "# BHK distribution\n",
    "if 'BHK' in df.columns:\n",
    "    print(\"\\nBHK distribution (percentage):\")\n",
    "    print(df['BHK'].value_counts(normalize=True) * 100)\n",
    "else:\n",
    "    print(\"\\nBHK column not found.\")\n",
    "\n",
    "# Property-Price summary\n",
    "if 'Property-Price' in df.columns:\n",
    "    print(\"\\nProperty-Price summary:\")\n",
    "    print(df['Property-Price'].describe())\n",
    "else:\n",
    "    print(\"\\nProperty-Price column not found.\")\n",
    "\n",
    "# Carpet-Construction-Area summary\n",
    "if 'Carpet-Construction-Area' in df.columns:\n",
    "    print(\"\\nCarpet-Construction-Area summary:\")\n",
    "    print(df['Carpet-Construction-Area'].describe())\n",
    "else:\n",
    "    print(\"\\nCarpet-Construction-Area column not found.\")\n",
    "\n",
    "# Service_Expiry_Date summary\n",
    "if 'Service_Expiry_Date' in df.columns:\n",
    "    df['Service_Expiry_Date'] = pd.to_datetime(df['Service_Expiry_Date'], errors='coerce')\n",
    "    print(\"\\nService_Expiry_Date summary:\")\n",
    "    print(df['Service_Expiry_Date'].describe())\n",
    "else:\n",
    "    print(\"\\nService_Expiry_Date column not found.\")\n",
    "\n",
    "# Commercial properties skew\n",
    "if property_type_col and 'Commercial-Property-Type' in df.columns:\n",
    "    commercial = df[df[property_type_col] == 'Commercial']\n",
    "    print(\"\\nCommercial Unmatched Properties:\", len(commercial))\n",
    "    print(commercial['Commercial-Property-Type'].value_counts())\n",
    "else:\n",
    "    print(\"\\nCommercial-Property-Type or Property_Type column not found.\")\n",
    "\n",
    "# Visualizations\n",
    "# Top 10 Locations\n",
    "if 'Location' in df.columns:\n",
    "    sns.countplot(y='Location', data=df, order=df['Location'].value_counts().index[:10])\n",
    "    plt.title('Top 10 Locations in Unmatched Properties')\n",
    "    plt.savefig('top_locations_unmatched.png')\n",
    "    plt.close()\n",
    "\n",
    "# Price Distribution by Property_Type\n",
    "if property_type_col and 'Property-Price' in df.columns:\n",
    "    sns.boxplot(x=property_type_col, y='Property-Price', data=df)\n",
    "    plt.title(f'Price Distribution by {property_type_col}')\n",
    "    plt.savefig('price_by_type_unmatched.png')\n",
    "    plt.close()\n",
    "\n",
    "# BHK Distribution\n",
    "if 'BHK' in df.columns:\n",
    "    sns.countplot(x='BHK', data=df)\n",
    "    plt.title('BHK Distribution in Unmatched Properties')\n",
    "    plt.savefig('bhk_distribution_unmatched.png')\n",
    "    plt.close()\n",
    "\n",
    "# Price vs. Area Scatter (if both columns exist)\n",
    "if 'Property-Price' in df.columns and 'Carpet-Construction-Area' in df.columns:\n",
    "    sns.scatterplot(x='Carpet-Construction-Area', y='Property-Price', hue=property_type_col if property_type_col else None, data=df)\n",
    "    plt.title('Price vs. Carpet Area in Unmatched Properties')\n",
    "    plt.savefig('price_vs_area_unmatched.png')\n",
    "    plt.close()\n",
    "\n",
    "print(\"\\nVisualizations saved to 'top_locations_unmatched.png', 'price_by_type_unmatched.png', 'bhk_distribution_unmatched.png', and 'price_vs_area_unmatched.png'\")\n",
    "\n",
    "print(\"\\nCommon Patterns Summary:\")\n",
    "print(\"- Skew in Property_Type: If >50% are Commercial, add Commercial-specific active properties or weight Commercial-Property-Type in the autoencoder.\")\n",
    "print(\"- Skew in Location: If Pune-based (P-) locations dominate (>30%), add Pune-specific active properties or relax max_distance_km to 10.0.\")\n",
    "print(\"- Price Range: If unmatched properties are in high/low price extremes (e.g., >100 or <10), increase price_tolerance to 0.60.\")\n",
    "print(\"- BHK or Area: If skewed (e.g., many 4+ BHK or >2000 sq.ft.), add interaction features (e.g., BHK * Carpet-Construction-Area) to numerical_cols.\")\n",
    "print(\"- Recent Expirations: If many recent Service_Expiry_Date (e.g., post-2024), prioritize them in matching by adding a recency weight to similarity.\")\n",
    "\n",
    "print(\"\\nModeling Recommendations:\")\n",
    "print(\"- Add missing locations (V-Nizampuera, V-Atladara, V-Aklkapuri) to geo_mapping.json with verified coordinates.\")\n",
    "print(\"- If Commercial skew: Use TargetEncoder for Commercial-Property-Type in run_data_pipeline to capture value-based encoding.\")\n",
    "print(\"- If Pune skew: Derive a 'City' feature (e.g., 'Pune' for P- locations) and add to categorical_cols.\")\n",
    "print(\"- To reduce unmatched (371 â†’ ~80): Set max_distance_km=10.0, price_tolerance=0.60, min_similarity_threshold=0.45, K=100 in find_similar_properties.\")\n",
    "print(\"- Retrain autoencoder with Dropout(0.2) after Dense layers and EarlyStopping(patience=20) if similarity rejections are high in rejection_reasons.csv.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "79358f67-163b-465d-b6ff-d03552fe34f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commercial-Property-Type\n",
      "-                  858\n",
      "Others               6\n",
      "Shops-Showrooms      5\n",
      "Office-Space         4\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('PropertyData.csv')\n",
    "print(df[df['Property_Type'] == 'Residential']['Commercial-Property-Type'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c46c6d-003c-4ac5-834b-bd73470695f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5fc9586e-4a81-4c31-b244-df609e8680ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kasindra: [22.8924, 72.4913]\n",
      "Vatva: [22.9664, 72.6159]\n",
      "Chandkheda: [23.1091, 72.5849]\n",
      "New Maninagar: [22.9857, 72.6432]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('geo_mapping.json', 'r') as f:\n",
    "    geo_mapping = json.load(f)\n",
    "print(\"Kasindra:\", geo_mapping.get('A-Kasindra'))\n",
    "print(\"Vatva:\", geo_mapping.get('A-Vatva'))\n",
    "print(\"Chandkheda:\", geo_mapping.get('A-Chandkheda'))\n",
    "print(\"New Maninagar:\", geo_mapping.get('A-New Maninagar'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f2cd4cb2-b329-4061-8a87-5fd8b5c7e5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"\n",
    "    Calculate the great-circle distance between two points on Earth using the Haversine formula.\n",
    "    Parameters:\n",
    "        lat1, lon1: Latitude and longitude of the first point (in degrees).\n",
    "        lat2, lon2: Latitude and longitude of the second point (in degrees).\n",
    "    Returns:\n",
    "        Distance in kilometers.\n",
    "    \"\"\"\n",
    "    # Convert degrees to radians\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    \n",
    "    # Haversine formula\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat / 2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    R = 6371  # Earth's radius in km\n",
    "    return c * R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "708a2a33-685a-44d9-b976-0405e121a130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kasindra to Vatva: 15.183162248921903 km\n",
      "Chandkheda to New Maninagar: 14.962025332545908 km\n",
      "Kasindra to Chandkheda: 25.930649912521414 km\n",
      "Vatva to New Maninagar: 3.523703832756868 km\n"
     ]
    }
   ],
   "source": [
    "print(\"Kasindra to Vatva:\", haversine(22.8924, 72.4913, 22.9664, 72.6159), \"km\")\n",
    "print(\"Chandkheda to New Maninagar:\", haversine(23.1091, 72.5849, 22.9857, 72.6432), \"km\")\n",
    "print(\"Kasindra to Chandkheda:\", haversine(22.8924, 72.4913, 23.1091, 72.5849), \"km\")\n",
    "print(\"Vatva to New Maninagar:\", haversine(22.9664, 72.6159, 22.9857, 72.6432), \"km\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9504761-c521-41dc-9c92-e1541e3f22d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Unmatched Properties: 371\n",
      "\n",
      "Property_Type distribution (percentage):\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Property_Type'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Property_Type'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Property_Type distribution\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mProperty_Type distribution (percentage):\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28mprint\u001b[39m(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProperty_Type\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue_counts(normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Location distribution (top 10)\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mLocation distribution (top 10):\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Property_Type'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the unmatched_expired_properties.csv file\n",
    "df = pd.read_csv('unmatched_expired_properties.csv')\n",
    "\n",
    "# Basic statistics\n",
    "print(\"Total Unmatched Properties:\", len(df))\n",
    "\n",
    "# Property_Type distribution\n",
    "print(\"\\nProperty_Type distribution (percentage):\")\n",
    "print(df['Property_Type'].value_counts(normalize=True) * 100)\n",
    "\n",
    "# Location distribution (top 10)\n",
    "print(\"\\nLocation distribution (top 10):\")\n",
    "print(df['Location'].value_counts().head(10))\n",
    "\n",
    "# Check for Pune-based locations (assuming Pune locations start with 'P-')\n",
    "pune_locations = df[df['Location'].str.startswith('P-', na=False)]\n",
    "print(\"\\nPune-based Unmatched Properties:\", len(pune_locations))\n",
    "print(pune_locations['Location'].value_counts())\n",
    "\n",
    "# BHK distribution\n",
    "print(\"\\nBHK distribution (percentage):\")\n",
    "print(df['BHK'].value_counts(normalize=True) * 100)\n",
    "\n",
    "# Property-Price summary\n",
    "print(\"\\nProperty-Price summary:\")\n",
    "print(df['Property-Price'].describe())\n",
    "\n",
    "# Carpet-Construction-Area summary\n",
    "print(\"\\nCarpet-Construction-Area summary:\")\n",
    "print(df['Carpet-Construction-Area'].describe())\n",
    "\n",
    "# Service_Expiry_Date summary (convert to datetime)\n",
    "df['Service_Expiry_Date'] = pd.to_datetime(df['Service_Expiry_Date'], errors='coerce')\n",
    "print(\"\\nService_Expiry_Date summary:\")\n",
    "print(df['Service_Expiry_Date'].describe())\n",
    "\n",
    "# Check for Commercial properties skew\n",
    "commercial = df[df['Property_Type'] == 'Commercial']\n",
    "print(\"\\nCommercial Unmatched Properties:\", len(commercial))\n",
    "print(commercial['Commercial-Property-Type'].value_counts() if 'Commercial-Property-Type' in df.columns else \"Commercial-Property-Type column not found\")\n",
    "\n",
    "# Visualizations\n",
    "# Top 10 Locations\n",
    "sns.countplot(y='Location', data=df, order=df['Location'].value_counts().index[:10])\n",
    "plt.title('Top 10 Locations in Unmatched Properties')\n",
    "plt.savefig('top_locations_unmatched.png')\n",
    "plt.close()\n",
    "\n",
    "# Price Distribution by Property_Type\n",
    "sns.boxplot(x='Property_Type', y='Property-Price', data=df)\n",
    "plt.title('Price Distribution by Property_Type')\n",
    "plt.savefig('price_by_type_unmatched.png')\n",
    "plt.close()\n",
    "\n",
    "# BHK Distribution\n",
    "sns.countplot(x='BHK', data=df)\n",
    "plt.title('BHK Distribution in Unmatched Properties')\n",
    "plt.savefig('bhk_distribution_unmatched.png')\n",
    "plt.close()\n",
    "\n",
    "print(\"\\nVisualizations saved to 'top_locations_unmatched.png', 'price_by_type_unmatched.png', and 'bhk_distribution_unmatched.png'\")\n",
    "print(\"\\nCommon Patterns Summary:\")\n",
    "print(\"- Skew in Property_Type: If >50% are Commercial, consider adding more Commercial-specific features (e.g., office amenities) to the autoencoder.\")\n",
    "print(\"- Skew in Location: If Pune-based (P-) locations dominate, relax filters for Pune or add Pune-specific active properties.\")\n",
    "print(\"- Price Range: If unmatched properties are in high/low price extremes, increase price_tolerance to 0.60 or add price-based clustering.\")\n",
    "print(\"- BHK or Area: If skewed (e.g., many 4+ BHK), add interaction features (e.g., BHK * Carpet-Construction-Area) to numerical_cols.\")\n",
    "print(\"- Recent Expirations: If many recent Service_Expiry_Date, prioritize them in matching (e.g., boost similarity for recent expirations).\")\n",
    "print(\"\\nModeling Recommendations:\")\n",
    "print(\"- If Commercial skew: Add Commercial-Property-Type as a weighted feature in the preprocessor (e.g., use TargetEncoder instead of OneHotEncoder for high-cardinality).\")\n",
    "print(\"- If Pune skew: Add a 'City' derived feature from Location (e.g., Pune vs. Ahmedabad) and include it in categorical_features.\")\n",
    "print(\"- To reduce unmatched overall: Increase K to 30, lower min_similarity_threshold to 0.5 in find_similar_properties, or use a hybrid similarity (e.g., 0.7*embedding_similarity + 0.3*price_similarity).\")\n",
    "print(\"- Retrain autoencoder with more epochs (e.g., patience=20 in EarlyStopping) if similarity rejections are high in rejection_reasons.csv.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c03a12-aece-4867-989e-f7bea24f2fe2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
